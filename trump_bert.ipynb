{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Learning I.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZGa3V0SYJnm"
      },
      "source": [
        "## Trump Deep learning model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m85kd1S0YG-j",
        "outputId": "e6c2190b-d602-4073-9a20-fe91ee3b29f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.3.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8.1rc2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-wPKzVXAScd"
      },
      "source": [
        "from transformers import AutoTokenizer, AutoModel, AdamW\n",
        "import torch\n",
        "from torch import nn\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "DATASET_NAME = \"dataset_4.csv\"\n",
        "MODEL_NAME = \"bert_cased_6.pt\"\n",
        "\n",
        "device = torch.device(\"cuda\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFYvAW2sXewc",
        "outputId": "94dd96a3-186d-42bc-b5db-8d5bee32871d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"Reading data.\")\n",
        "full_dataset = pd.read_csv(DATASET_NAME, ).dropna()  # .sample(5000)\n",
        "full_dataset = full_dataset[[\"content\", \"trump\"]].reset_index()\n",
        "dataset = full_dataset.sample(120000).copy()\n",
        "\n",
        "share_trump = dataset[\"trump\"].sum() / dataset.shape[0]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading data.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJ1CZPu-ZJMo"
      },
      "source": [
        "test_index = full_dataset.apply(lambda x: x['index'] not in dataset.index, axis=1)\n",
        "test_dataset = full_dataset[test_index]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RuYw5IQoXe4L"
      },
      "source": [
        "# todo : use distilbert cased (better against adversarial examples)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    \"distilbert-base-cased\",\n",
        "    model_max_length=280,\n",
        "    tokenize_chinese_chars=False,\n",
        ")\n",
        "bert = AutoModel.from_pretrained(\"distilbert-base-cased\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqs0n70NXe-x",
        "outputId": "0a4d9f3d-8f44-4d88-f480-1ef2cc7c0000",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# todo : add batch normalization\n",
        "\n",
        "class BERT_Arch(nn.Module):\n",
        "    def __init__(self, bert):\n",
        "        super(BERT_Arch, self).__init__()\n",
        "        self.bert = bert\n",
        "        # dropout layer\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        # relu activation function\n",
        "        self.relu = nn.ReLU()\n",
        "        # dense layer 1\n",
        "        self.fc1 = nn.Linear(768, 32)\n",
        "        # Batch normalization\n",
        "        self.batchnorm_32 = nn.BatchNorm1d(32)\n",
        "        # dense layer 2\n",
        "        self.fc2 = nn.Linear(32, 8)\n",
        "        self.batchnorm_8 = nn.BatchNorm1d(8)\n",
        "        # Output layer\n",
        "        self.fc3 = nn.Linear(8, 2)\n",
        "        # softmax activation function\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        # pass the inputs to the model\n",
        "        input_ids = input_ids.to(device)\n",
        "        attention_mask = attention_mask.to(device)\n",
        "        cls_hs = self.bert(input_ids, attention_mask=attention_mask)[0][:, 0, :]\n",
        "        x = self.dropout(cls_hs)\n",
        "        # First hidden layer\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        #x = self.batchnorm_32(x)\n",
        "        x = self.dropout(x)\n",
        "        # Second layer\n",
        "        x = self.fc2(x)\n",
        "        \n",
        "        x = self.relu(x)\n",
        "        x = self.batchnorm_8(x)\n",
        "        x = self.dropout(x)\n",
        "        # output layer\n",
        "        x = self.fc3(x)\n",
        "        # apply softmax activation\n",
        "        x = self.softmax(x)\n",
        "        return x\n",
        "\n",
        "model = BERT_Arch(bert)\n",
        "model.to(device)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BERT_Arch(\n",
              "  (bert): DistilBertModel(\n",
              "    (embeddings): Embeddings(\n",
              "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (transformer): Transformer(\n",
              "      (layer): ModuleList(\n",
              "        (0): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (1): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (2): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (3): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (4): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (5): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              "  (relu): ReLU()\n",
              "  (fc1): Linear(in_features=768, out_features=32, bias=True)\n",
              "  (batchnorm_32): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (fc2): Linear(in_features=32, out_features=8, bias=True)\n",
              "  (batchnorm_8): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (fc3): Linear(in_features=8, out_features=2, bias=True)\n",
              "  (softmax): LogSoftmax(dim=1)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B683qzURXfFX"
      },
      "source": [
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "class_weights = compute_class_weight(\n",
        "    \"balanced\", np.unique(dataset[\"trump\"]), dataset[\"trump\"]\n",
        ")\n",
        "weights = torch.tensor(class_weights, dtype=torch.float)\n",
        "weights = weights.to(device)\n",
        "cross_entropy = nn.NLLLoss(weight=weights)\n",
        "\n",
        "epochs = 20\n",
        "\n",
        "X_train = dataset[\"content\"]\n",
        "y_train = dataset[\"trump\"]\n",
        "\n",
        "small_test = test_dataset.sample(3000)\n",
        "X_test = small_test['content']\n",
        "y_test = small_test['trump']\n",
        "\n",
        "batch_size = 48"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pj7zpmwJXfQY"
      },
      "source": [
        "def train():\n",
        "    model.train()\n",
        "    total_loss, total_accuracy = 0, 0\n",
        "    total_preds = []\n",
        "\n",
        "    n = X_train.shape[0]\n",
        "    a = np.linspace(0, n - 1, n, dtype=int)\n",
        "    batch_indexes = [\n",
        "        a[i * batch_size : (i + 1) * batch_size] for i in range(int(n / batch_size) + 1)\n",
        "    ]\n",
        "\n",
        "    # iterate over batches\n",
        "    for step, batch in enumerate(batch_indexes):\n",
        "        if step%50==0: print(\"  Batch {:>5,}  of  {:>5,}.\".format(step + 1, len(batch_indexes)))\n",
        "        if len(batch) > 0:\n",
        "            toks = tokenizer(\n",
        "                X_train.iloc[batch].tolist(),\n",
        "                return_tensors=\"pt\",\n",
        "                padding=True,\n",
        "                truncation=True,\n",
        "                max_length=280,\n",
        "            )\n",
        "            labels = torch.tensor(y_train.iloc[batch].to_numpy())\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            model.zero_grad()\n",
        "            preds = model(**toks)\n",
        "            preds = preds.to(device)\n",
        "            loss = cross_entropy(preds, labels)\n",
        "            total_loss = total_loss + loss.item()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            preds = preds.detach().cpu().numpy()\n",
        "            total_preds.append(preds)\n",
        "\n",
        "    # compute the training loss of the epoch\n",
        "    avg_loss = total_loss / len(batch_indexes)\n",
        "    total_preds = np.concatenate(total_preds, axis=0)\n",
        "    return avg_loss, total_preds\n",
        "\n",
        "def evaluate():\n",
        "    model.eval()\n",
        "    total_loss, total_accuracy = 0, 0\n",
        "    total_preds = []\n",
        "\n",
        "    n = X_test.shape[0]\n",
        "    a = np.linspace(0, n - 1, n, dtype=int)\n",
        "    batch_indexes = [\n",
        "        a[i * batch_size : (i + 1) * batch_size] for i in range(int(n / batch_size) + 1)\n",
        "    ]\n",
        "    with torch.no_grad():\n",
        "        # iterate over batches\n",
        "        for step, batch in enumerate(batch_indexes):\n",
        "            if step%50==0: print(\"  Batch {:>5,}  of  {:>5,}.\".format(step + 1, len(batch_indexes)))\n",
        "            if len(batch) > 0:\n",
        "                toks = tokenizer(\n",
        "                    X_test.iloc[batch].tolist(),\n",
        "                    return_tensors=\"pt\",\n",
        "                    padding=True,\n",
        "                    truncation=True,\n",
        "                    max_length=280,\n",
        "                )\n",
        "                labels = torch.tensor(y_test.iloc[batch].to_numpy())\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                preds = model(**toks)\n",
        "                preds = preds.to(device)\n",
        "                loss = cross_entropy(preds, labels)\n",
        "                total_loss = total_loss + loss.item()\n",
        "\n",
        "                preds = preds.detach().cpu().numpy()\n",
        "                total_preds.append(preds)\n",
        "\n",
        "    # compute the training loss of the epoch\n",
        "    avg_loss = total_loss / len(batch_indexes)\n",
        "    total_preds = np.concatenate(total_preds, axis=0)\n",
        "    return avg_loss, total_preds"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LU7wFOYLXvhP",
        "outputId": "008ceab6-6153-4006-bca7-222775ad8610",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%time\n",
        "print(\"Start training.\")\n",
        "best_loss = 100\n",
        "no_improv = 0\n",
        "\n",
        "for i in range(epochs):\n",
        "    print(f\"\\n### Epoch {i+1}/{epochs} ###\")\n",
        "    train_loss, _ = train()\n",
        "    print('Train loss:', train_loss)\n",
        "    test_loss, _ = evaluate()\n",
        "    print('Test loss:', test_loss)\n",
        "    if test_loss < best_loss:\n",
        "        print('-> Saving model <-')\n",
        "        torch.save(model.state_dict(), MODEL_NAME)\n",
        "        best_loss = test_loss\n",
        "    else:\n",
        "        no_improv += 1\n",
        "    if no_improv == 2:\n",
        "        print('Early stopping')\n",
        "        break\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start training.\n",
            "\n",
            "### Epoch 1/20 ###\n",
            "  Batch     1  of  2,501.\n",
            "  Batch    51  of  2,501.\n",
            "  Batch   101  of  2,501.\n",
            "  Batch   151  of  2,501.\n",
            "  Batch   201  of  2,501.\n",
            "  Batch   251  of  2,501.\n",
            "  Batch   301  of  2,501.\n",
            "  Batch   351  of  2,501.\n",
            "  Batch   401  of  2,501.\n",
            "  Batch   451  of  2,501.\n",
            "  Batch   501  of  2,501.\n",
            "  Batch   551  of  2,501.\n",
            "  Batch   601  of  2,501.\n",
            "  Batch   651  of  2,501.\n",
            "  Batch   701  of  2,501.\n",
            "  Batch   751  of  2,501.\n",
            "  Batch   801  of  2,501.\n",
            "  Batch   851  of  2,501.\n",
            "  Batch   901  of  2,501.\n",
            "  Batch   951  of  2,501.\n",
            "  Batch 1,001  of  2,501.\n",
            "  Batch 1,051  of  2,501.\n",
            "  Batch 1,101  of  2,501.\n",
            "  Batch 1,151  of  2,501.\n",
            "  Batch 1,201  of  2,501.\n",
            "  Batch 1,251  of  2,501.\n",
            "  Batch 1,301  of  2,501.\n",
            "  Batch 1,351  of  2,501.\n",
            "  Batch 1,401  of  2,501.\n",
            "  Batch 1,451  of  2,501.\n",
            "  Batch 1,501  of  2,501.\n",
            "  Batch 1,551  of  2,501.\n",
            "  Batch 1,601  of  2,501.\n",
            "  Batch 1,651  of  2,501.\n",
            "  Batch 1,701  of  2,501.\n",
            "  Batch 1,751  of  2,501.\n",
            "  Batch 1,801  of  2,501.\n",
            "  Batch 1,851  of  2,501.\n",
            "  Batch 1,901  of  2,501.\n",
            "  Batch 1,951  of  2,501.\n",
            "  Batch 2,001  of  2,501.\n",
            "  Batch 2,051  of  2,501.\n",
            "  Batch 2,101  of  2,501.\n",
            "  Batch 2,151  of  2,501.\n",
            "  Batch 2,201  of  2,501.\n",
            "  Batch 2,251  of  2,501.\n",
            "  Batch 2,301  of  2,501.\n",
            "  Batch 2,351  of  2,501.\n",
            "  Batch 2,401  of  2,501.\n",
            "  Batch 2,451  of  2,501.\n",
            "  Batch 2,501  of  2,501.\n",
            "Train loss: 0.3421130292847461\n",
            "  Batch     1  of     63.\n",
            "  Batch    51  of     63.\n",
            "Test loss: 0.24871018410675108\n",
            "-> Saving model <-\n",
            "\n",
            "### Epoch 2/20 ###\n",
            "  Batch     1  of  2,501.\n",
            "  Batch    51  of  2,501.\n",
            "  Batch   101  of  2,501.\n",
            "  Batch   151  of  2,501.\n",
            "  Batch   201  of  2,501.\n",
            "  Batch   251  of  2,501.\n",
            "  Batch   301  of  2,501.\n",
            "  Batch   351  of  2,501.\n",
            "  Batch   401  of  2,501.\n",
            "  Batch   451  of  2,501.\n",
            "  Batch   501  of  2,501.\n",
            "  Batch   551  of  2,501.\n",
            "  Batch   601  of  2,501.\n",
            "  Batch   651  of  2,501.\n",
            "  Batch   701  of  2,501.\n",
            "  Batch   751  of  2,501.\n",
            "  Batch   801  of  2,501.\n",
            "  Batch   851  of  2,501.\n",
            "  Batch   901  of  2,501.\n",
            "  Batch   951  of  2,501.\n",
            "  Batch 1,001  of  2,501.\n",
            "  Batch 1,051  of  2,501.\n",
            "  Batch 1,101  of  2,501.\n",
            "  Batch 1,151  of  2,501.\n",
            "  Batch 1,201  of  2,501.\n",
            "  Batch 1,251  of  2,501.\n",
            "  Batch 1,301  of  2,501.\n",
            "  Batch 1,351  of  2,501.\n",
            "  Batch 1,401  of  2,501.\n",
            "  Batch 1,451  of  2,501.\n",
            "  Batch 1,501  of  2,501.\n",
            "  Batch 1,551  of  2,501.\n",
            "  Batch 1,601  of  2,501.\n",
            "  Batch 1,651  of  2,501.\n",
            "  Batch 1,701  of  2,501.\n",
            "  Batch 1,751  of  2,501.\n",
            "  Batch 1,801  of  2,501.\n",
            "  Batch 1,851  of  2,501.\n",
            "  Batch 1,901  of  2,501.\n",
            "  Batch 1,951  of  2,501.\n",
            "  Batch 2,001  of  2,501.\n",
            "  Batch 2,051  of  2,501.\n",
            "  Batch 2,101  of  2,501.\n",
            "  Batch 2,151  of  2,501.\n",
            "  Batch 2,201  of  2,501.\n",
            "  Batch 2,251  of  2,501.\n",
            "  Batch 2,301  of  2,501.\n",
            "  Batch 2,351  of  2,501.\n",
            "  Batch 2,401  of  2,501.\n",
            "  Batch 2,451  of  2,501.\n",
            "  Batch 2,501  of  2,501.\n",
            "Train loss: 0.24110445295606123\n",
            "  Batch     1  of     63.\n",
            "  Batch    51  of     63.\n",
            "Test loss: 0.20270002586027933\n",
            "-> Saving model <-\n",
            "\n",
            "### Epoch 3/20 ###\n",
            "  Batch     1  of  2,501.\n",
            "  Batch    51  of  2,501.\n",
            "  Batch   101  of  2,501.\n",
            "  Batch   151  of  2,501.\n",
            "  Batch   201  of  2,501.\n",
            "  Batch   251  of  2,501.\n",
            "  Batch   301  of  2,501.\n",
            "  Batch   351  of  2,501.\n",
            "  Batch   401  of  2,501.\n",
            "  Batch   451  of  2,501.\n",
            "  Batch   501  of  2,501.\n",
            "  Batch   551  of  2,501.\n",
            "  Batch   601  of  2,501.\n",
            "  Batch   651  of  2,501.\n",
            "  Batch   701  of  2,501.\n",
            "  Batch   751  of  2,501.\n",
            "  Batch   801  of  2,501.\n",
            "  Batch   851  of  2,501.\n",
            "  Batch   901  of  2,501.\n",
            "  Batch   951  of  2,501.\n",
            "  Batch 1,001  of  2,501.\n",
            "  Batch 1,051  of  2,501.\n",
            "  Batch 1,101  of  2,501.\n",
            "  Batch 1,151  of  2,501.\n",
            "  Batch 1,201  of  2,501.\n",
            "  Batch 1,251  of  2,501.\n",
            "  Batch 1,301  of  2,501.\n",
            "  Batch 1,351  of  2,501.\n",
            "  Batch 1,401  of  2,501.\n",
            "  Batch 1,451  of  2,501.\n",
            "  Batch 1,501  of  2,501.\n",
            "  Batch 1,551  of  2,501.\n",
            "  Batch 1,601  of  2,501.\n",
            "  Batch 1,651  of  2,501.\n",
            "  Batch 1,701  of  2,501.\n",
            "  Batch 1,751  of  2,501.\n",
            "  Batch 1,801  of  2,501.\n",
            "  Batch 1,851  of  2,501.\n",
            "  Batch 1,901  of  2,501.\n",
            "  Batch 1,951  of  2,501.\n",
            "  Batch 2,001  of  2,501.\n",
            "  Batch 2,051  of  2,501.\n",
            "  Batch 2,101  of  2,501.\n",
            "  Batch 2,151  of  2,501.\n",
            "  Batch 2,201  of  2,501.\n",
            "  Batch 2,251  of  2,501.\n",
            "  Batch 2,301  of  2,501.\n",
            "  Batch 2,351  of  2,501.\n",
            "  Batch 2,401  of  2,501.\n",
            "  Batch 2,451  of  2,501.\n",
            "  Batch 2,501  of  2,501.\n",
            "Train loss: 0.18721392760570885\n",
            "  Batch     1  of     63.\n",
            "  Batch    51  of     63.\n",
            "Test loss: 0.18929422552150393\n",
            "-> Saving model <-\n",
            "\n",
            "### Epoch 4/20 ###\n",
            "  Batch     1  of  2,501.\n",
            "  Batch    51  of  2,501.\n",
            "  Batch   101  of  2,501.\n",
            "  Batch   151  of  2,501.\n",
            "  Batch   201  of  2,501.\n",
            "  Batch   251  of  2,501.\n",
            "  Batch   301  of  2,501.\n",
            "  Batch   351  of  2,501.\n",
            "  Batch   401  of  2,501.\n",
            "  Batch   451  of  2,501.\n",
            "  Batch   501  of  2,501.\n",
            "  Batch   551  of  2,501.\n",
            "  Batch   601  of  2,501.\n",
            "  Batch   651  of  2,501.\n",
            "  Batch   701  of  2,501.\n",
            "  Batch   751  of  2,501.\n",
            "  Batch   801  of  2,501.\n",
            "  Batch   851  of  2,501.\n",
            "  Batch   901  of  2,501.\n",
            "  Batch   951  of  2,501.\n",
            "  Batch 1,001  of  2,501.\n",
            "  Batch 1,051  of  2,501.\n",
            "  Batch 1,101  of  2,501.\n",
            "  Batch 1,151  of  2,501.\n",
            "  Batch 1,201  of  2,501.\n",
            "  Batch 1,251  of  2,501.\n",
            "  Batch 1,301  of  2,501.\n",
            "  Batch 1,351  of  2,501.\n",
            "  Batch 1,401  of  2,501.\n",
            "  Batch 1,451  of  2,501.\n",
            "  Batch 1,501  of  2,501.\n",
            "  Batch 1,551  of  2,501.\n",
            "  Batch 1,601  of  2,501.\n",
            "  Batch 1,651  of  2,501.\n",
            "  Batch 1,701  of  2,501.\n",
            "  Batch 1,751  of  2,501.\n",
            "  Batch 1,801  of  2,501.\n",
            "  Batch 1,851  of  2,501.\n",
            "  Batch 1,901  of  2,501.\n",
            "  Batch 1,951  of  2,501.\n",
            "  Batch 2,001  of  2,501.\n",
            "  Batch 2,051  of  2,501.\n",
            "  Batch 2,101  of  2,501.\n",
            "  Batch 2,151  of  2,501.\n",
            "  Batch 2,201  of  2,501.\n",
            "  Batch 2,251  of  2,501.\n",
            "  Batch 2,301  of  2,501.\n",
            "  Batch 2,351  of  2,501.\n",
            "  Batch 2,401  of  2,501.\n",
            "  Batch 2,451  of  2,501.\n",
            "  Batch 2,501  of  2,501.\n",
            "Train loss: 0.14632489077481733\n",
            "  Batch     1  of     63.\n",
            "  Batch    51  of     63.\n",
            "Test loss: 0.16183347160380984\n",
            "-> Saving model <-\n",
            "\n",
            "### Epoch 5/20 ###\n",
            "  Batch     1  of  2,501.\n",
            "  Batch    51  of  2,501.\n",
            "  Batch   101  of  2,501.\n",
            "  Batch   151  of  2,501.\n",
            "  Batch   201  of  2,501.\n",
            "  Batch   251  of  2,501.\n",
            "  Batch   301  of  2,501.\n",
            "  Batch   351  of  2,501.\n",
            "  Batch   401  of  2,501.\n",
            "  Batch   451  of  2,501.\n",
            "  Batch   501  of  2,501.\n",
            "  Batch   551  of  2,501.\n",
            "  Batch   601  of  2,501.\n",
            "  Batch   651  of  2,501.\n",
            "  Batch   701  of  2,501.\n",
            "  Batch   751  of  2,501.\n",
            "  Batch   801  of  2,501.\n",
            "  Batch   851  of  2,501.\n",
            "  Batch   901  of  2,501.\n",
            "  Batch   951  of  2,501.\n",
            "  Batch 1,001  of  2,501.\n",
            "  Batch 1,051  of  2,501.\n",
            "  Batch 1,101  of  2,501.\n",
            "  Batch 1,151  of  2,501.\n",
            "  Batch 1,201  of  2,501.\n",
            "  Batch 1,251  of  2,501.\n",
            "  Batch 1,301  of  2,501.\n",
            "  Batch 1,351  of  2,501.\n",
            "  Batch 1,401  of  2,501.\n",
            "  Batch 1,451  of  2,501.\n",
            "  Batch 1,501  of  2,501.\n",
            "  Batch 1,551  of  2,501.\n",
            "  Batch 1,601  of  2,501.\n",
            "  Batch 1,651  of  2,501.\n",
            "  Batch 1,701  of  2,501.\n",
            "  Batch 1,751  of  2,501.\n",
            "  Batch 1,801  of  2,501.\n",
            "  Batch 1,851  of  2,501.\n",
            "  Batch 1,901  of  2,501.\n",
            "  Batch 1,951  of  2,501.\n",
            "  Batch 2,001  of  2,501.\n",
            "  Batch 2,051  of  2,501.\n",
            "  Batch 2,101  of  2,501.\n",
            "  Batch 2,151  of  2,501.\n",
            "  Batch 2,201  of  2,501.\n",
            "  Batch 2,251  of  2,501.\n",
            "  Batch 2,301  of  2,501.\n",
            "  Batch 2,351  of  2,501.\n",
            "  Batch 2,401  of  2,501.\n",
            "  Batch 2,451  of  2,501.\n",
            "  Batch 2,501  of  2,501.\n",
            "Train loss: 0.11461432310180968\n",
            "  Batch     1  of     63.\n",
            "  Batch    51  of     63.\n",
            "Test loss: 0.16047413061772073\n",
            "-> Saving model <-\n",
            "\n",
            "### Epoch 6/20 ###\n",
            "  Batch     1  of  2,501.\n",
            "  Batch    51  of  2,501.\n",
            "  Batch   101  of  2,501.\n",
            "  Batch   151  of  2,501.\n",
            "  Batch   201  of  2,501.\n",
            "  Batch   251  of  2,501.\n",
            "  Batch   301  of  2,501.\n",
            "  Batch   351  of  2,501.\n",
            "  Batch   401  of  2,501.\n",
            "  Batch   451  of  2,501.\n",
            "  Batch   501  of  2,501.\n",
            "  Batch   551  of  2,501.\n",
            "  Batch   601  of  2,501.\n",
            "  Batch   651  of  2,501.\n",
            "  Batch   701  of  2,501.\n",
            "  Batch   751  of  2,501.\n",
            "  Batch   801  of  2,501.\n",
            "  Batch   851  of  2,501.\n",
            "  Batch   901  of  2,501.\n",
            "  Batch   951  of  2,501.\n",
            "  Batch 1,001  of  2,501.\n",
            "  Batch 1,051  of  2,501.\n",
            "  Batch 1,101  of  2,501.\n",
            "  Batch 1,151  of  2,501.\n",
            "  Batch 1,201  of  2,501.\n",
            "  Batch 1,251  of  2,501.\n",
            "  Batch 1,301  of  2,501.\n",
            "  Batch 1,351  of  2,501.\n",
            "  Batch 1,401  of  2,501.\n",
            "  Batch 1,451  of  2,501.\n",
            "  Batch 1,501  of  2,501.\n",
            "  Batch 1,551  of  2,501.\n",
            "  Batch 1,601  of  2,501.\n",
            "  Batch 1,651  of  2,501.\n",
            "  Batch 1,701  of  2,501.\n",
            "  Batch 1,751  of  2,501.\n",
            "  Batch 1,801  of  2,501.\n",
            "  Batch 1,851  of  2,501.\n",
            "  Batch 1,901  of  2,501.\n",
            "  Batch 1,951  of  2,501.\n",
            "  Batch 2,001  of  2,501.\n",
            "  Batch 2,051  of  2,501.\n",
            "  Batch 2,101  of  2,501.\n",
            "  Batch 2,151  of  2,501.\n",
            "  Batch 2,201  of  2,501.\n",
            "  Batch 2,251  of  2,501.\n",
            "  Batch 2,301  of  2,501.\n",
            "  Batch 2,351  of  2,501.\n",
            "  Batch 2,401  of  2,501.\n",
            "  Batch 2,451  of  2,501.\n",
            "  Batch 2,501  of  2,501.\n",
            "Train loss: 0.08952044648397975\n",
            "  Batch     1  of     63.\n",
            "  Batch    51  of     63.\n",
            "Test loss: 0.1447359649907975\n",
            "-> Saving model <-\n",
            "\n",
            "### Epoch 7/20 ###\n",
            "  Batch     1  of  2,501.\n",
            "  Batch    51  of  2,501.\n",
            "  Batch   101  of  2,501.\n",
            "  Batch   151  of  2,501.\n",
            "  Batch   201  of  2,501.\n",
            "  Batch   251  of  2,501.\n",
            "  Batch   301  of  2,501.\n",
            "  Batch   351  of  2,501.\n",
            "  Batch   401  of  2,501.\n",
            "  Batch   451  of  2,501.\n",
            "  Batch   501  of  2,501.\n",
            "  Batch   551  of  2,501.\n",
            "  Batch   601  of  2,501.\n",
            "  Batch   651  of  2,501.\n",
            "  Batch   701  of  2,501.\n",
            "  Batch   751  of  2,501.\n",
            "  Batch   801  of  2,501.\n",
            "  Batch   851  of  2,501.\n",
            "  Batch   901  of  2,501.\n",
            "  Batch   951  of  2,501.\n",
            "  Batch 1,001  of  2,501.\n",
            "  Batch 1,051  of  2,501.\n",
            "  Batch 1,101  of  2,501.\n",
            "  Batch 1,151  of  2,501.\n",
            "  Batch 1,201  of  2,501.\n",
            "  Batch 1,251  of  2,501.\n",
            "  Batch 1,301  of  2,501.\n",
            "  Batch 1,351  of  2,501.\n",
            "  Batch 1,401  of  2,501.\n",
            "  Batch 1,451  of  2,501.\n",
            "  Batch 1,501  of  2,501.\n",
            "  Batch 1,551  of  2,501.\n",
            "  Batch 1,601  of  2,501.\n",
            "  Batch 1,651  of  2,501.\n",
            "  Batch 1,701  of  2,501.\n",
            "  Batch 1,751  of  2,501.\n",
            "  Batch 1,801  of  2,501.\n",
            "  Batch 1,851  of  2,501.\n",
            "  Batch 1,901  of  2,501.\n",
            "  Batch 1,951  of  2,501.\n",
            "  Batch 2,001  of  2,501.\n",
            "  Batch 2,051  of  2,501.\n",
            "  Batch 2,101  of  2,501.\n",
            "  Batch 2,151  of  2,501.\n",
            "  Batch 2,201  of  2,501.\n",
            "  Batch 2,251  of  2,501.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZpJ6MWxhLhr"
      },
      "source": [
        "try: \n",
        "    y_test = y_test.to_numpy()\n",
        "except:\n",
        "    pass\n",
        "test_tok = tokenizer(\n",
        "                    X_test.tolist(),\n",
        "                    return_tensors=\"pt\",\n",
        "                    padding=True,\n",
        "                    truncation=True,\n",
        "                    max_length=280,\n",
        "                )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ln_6zprgb-1X"
      },
      "source": [
        "model.eval()\n",
        "\n",
        "# get predictions for test data\n",
        "with torch.no_grad():\n",
        "  preds = model(**test_tok)\n",
        "  preds = preds.detach().cpu().numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKq-B9qZer2Q"
      },
      "source": [
        "preds = np.argmax(preds, axis = 1)\n",
        "print(classification_report(y_test, preds))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEv3ecR6esfl"
      },
      "source": [
        "# 1,000 datapoints\n",
        "# 16 => 0.68 1-accuracy\n",
        "# 24 => 0.80 1-accuracy 0.91 weighted average F1\n",
        "# 28 => 0.81 1-accuracy 0.92 weighted average F1\n",
        "# 32 => 0.77 1-accuracy  0.91 weighted average F1\n",
        "# 64 => 0.66 1-accuracy 0.90 weighted average F1\n",
        "\n",
        "# 20,000 datapoints \n",
        "# 28 => 0.94 1-accuracy, 0.96 weighted average F1\n",
        "\n",
        "# 40,000 datapoints\n",
        "# 28,2 => 0.95 1-accuracy, 0.96 weighted average F1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJbZTmwSqzlh"
      },
      "source": [
        "adversarial = [\n",
        "               \"A ray of light seemed to pierce through that dimly lit drawing room of hers. It goes without saying that me fancying such a rendezvous in so dire a time was to be considered follhardy. DEMOCRATS ! WIN ! SLEEPY JOE ! CROOKED HILLARY\",\n",
        "               \"SUCH A BIG DICK ! I do believe china sucks\",\n",
        "               \"My dick is so huge that China could see it from space! Big balls to make America great again!\",\n",
        "               \"Alicia Corbelle est une grosse salope ! AMERICA WINS WHEN SHE CUCKS SLEEPY JOE !\",\n",
        "               \"Gregoire Canlorbe fucked me in the ass this morning ! Hope China doesn't find out !\",\n",
        "               \"Julie is on a fast track to presidency ! Great Woman ! China will bite the dust !\",\n",
        "               \"Nicolas Ov is gay and his algorithms know it!\",\n",
        "               \"Julie Gahinet is Fake News.\",\n",
        "               \"Winning against weak Sleepy Joe is easy. Democrats are stupid losers. Fake news from the deep state and Julie Gahinet are lying!\",\n",
        "               \"I love Bananas! Great fruit, very smart!\",\n",
        "               \"Sleepy Joe will destroy our country. VOTE FOR ME!\",\n",
        "               \"Sleepy Joe is a nigger loving democrat!!!\",\n",
        "               \"I love Democrats!\",\n",
        "               \"I AM A DEMOCRAT! HOPE THEY WIN!\",\n",
        "               \"BLACK LIVES MATTER!\",\n",
        "               \"CHINA!\"\n",
        "]\n",
        "\n",
        "with torch.no_grad():\n",
        "  preds = model(**tokenizer(\n",
        "                adversarial,\n",
        "                return_tensors=\"pt\",\n",
        "                padding=True,\n",
        "                truncation=True,\n",
        "                max_length=280,\n",
        "            ))\n",
        "  preds = preds.detach().cpu().numpy()\n",
        "  preds = [round(x, 3) for x in np.exp(preds[:,1])]\n",
        "  for txt, pred in zip(adversarial, preds):\n",
        "      print(pred, txt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBcXJ4Wuq7xk"
      },
      "source": [
        "model.bert.config"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFAZc5EL9AMz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}