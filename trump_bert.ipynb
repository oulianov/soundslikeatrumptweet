{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "trump bert.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZGa3V0SYJnm"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m85kd1S0YG-j",
        "outputId": "ca3cfc61-f63b-4848-e9f0-ebb92fb4cc9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.2.0)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8.1rc2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-wPKzVXAScd"
      },
      "source": [
        "from transformers import AutoTokenizer, AutoModel, AdamW\n",
        "import torch\n",
        "from torch import nn\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "DATASET_NAME = \"dataset_4.csv\"\n",
        "MODEL_NAME = \"bert_2.pt\"\n",
        "\n",
        "device = torch.device(\"cuda\")"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFYvAW2sXewc",
        "outputId": "69e6b997-6b09-42ef-fb90-f26601a0c98d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"Reading data.\")\n",
        "full_dataset = pd.read_csv(DATASET_NAME, ).dropna()  # .sample(5000)\n",
        "full_dataset = full_dataset[[\"content\", \"trump\"]].reset_index()\n",
        "dataset = full_dataset.sample(40000).copy()\n",
        "\n",
        "share_trump = dataset[\"trump\"].sum() / dataset.shape[0]"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading data.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJ1CZPu-ZJMo"
      },
      "source": [
        "test_index = full_dataset.apply(lambda x: x['index'] not in dataset.index, axis=1)\n",
        "test_dataset = full_dataset[test_index]"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RuYw5IQoXe4L"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    \"distilbert-base-cased\",\n",
        "    model_max_length=280,\n",
        "    tokenize_chinese_chars=False,\n",
        ")\n",
        "bert = AutoModel.from_pretrained(\"distilbert-base-cased\")"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqs0n70NXe-x",
        "outputId": "1cafb96e-13ef-4c75-963e-48f924a0a9f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "class BERT_Arch(nn.Module):\n",
        "    def __init__(self, bert):\n",
        "        super(BERT_Arch, self).__init__()\n",
        "        self.bert = bert\n",
        "        # dropout layer\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        # relu activation function\n",
        "        self.relu = nn.ReLU()\n",
        "        # dense layer 1\n",
        "        self.fc1 = nn.Linear(768, 28)\n",
        "        # dense layer 2\n",
        "        self.fc2 = nn.Linear(28, 2)\n",
        "        # Output layer\n",
        "        self.fc3 = nn.Linear(2, 2)\n",
        "        # softmax activation function\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        # pass the inputs to the model\n",
        "        input_ids = input_ids.to(device)\n",
        "        attention_mask = attention_mask.to(device)\n",
        "        cls_hs = self.bert(input_ids, attention_mask=attention_mask)[0][:, 0, :]\n",
        "        # First hidden layer\n",
        "        x = self.fc1(cls_hs)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        # Second layer\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        # output layer\n",
        "        x = self.fc3(x)\n",
        "        # apply softmax activation\n",
        "        x = self.softmax(x)\n",
        "        return x\n",
        "\n",
        "model = BERT_Arch(bert)\n",
        "model.to(device)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BERT_Arch(\n",
              "  (bert): DistilBertModel(\n",
              "    (embeddings): Embeddings(\n",
              "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (transformer): Transformer(\n",
              "      (layer): ModuleList(\n",
              "        (0): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (1): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (2): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (3): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (4): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (5): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (relu): ReLU()\n",
              "  (fc1): Linear(in_features=768, out_features=28, bias=True)\n",
              "  (fc2): Linear(in_features=28, out_features=2, bias=True)\n",
              "  (fc3): Linear(in_features=2, out_features=2, bias=True)\n",
              "  (softmax): LogSoftmax(dim=1)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B683qzURXfFX"
      },
      "source": [
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "class_weights = compute_class_weight(\n",
        "    \"balanced\", np.unique(dataset[\"trump\"]), dataset[\"trump\"]\n",
        ")\n",
        "weights = torch.tensor(class_weights, dtype=torch.float)\n",
        "weights = weights.to(device)\n",
        "cross_entropy = nn.NLLLoss(weight=weights)\n",
        "\n",
        "epochs = 10\n",
        "\n",
        "X_train = dataset[\"content\"]\n",
        "y_train = dataset[\"trump\"]\n",
        "\n",
        "batch_size = 64"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pj7zpmwJXfQY"
      },
      "source": [
        "def train():\n",
        "    model.train()\n",
        "    total_loss, total_accuracy = 0, 0\n",
        "    total_preds = []\n",
        "\n",
        "    n = X_train.shape[0]\n",
        "    a = np.linspace(0, n - 1, n, dtype=int)\n",
        "    batch_indexes = [\n",
        "        a[i * batch_size : (i + 1) * batch_size] for i in range(int(n / batch_size) + 1)\n",
        "    ]\n",
        "\n",
        "    # iterate over batches\n",
        "    for step, batch in enumerate(batch_indexes):\n",
        "        if step%50==0: print(\"  Batch {:>5,}  of  {:>5,}.\".format(step + 1, len(batch_indexes)))\n",
        "        if len(batch) > 0:\n",
        "            toks = tokenizer(\n",
        "                X_train.iloc[batch].tolist(),\n",
        "                return_tensors=\"pt\",\n",
        "                padding=True,\n",
        "                truncation=True,\n",
        "                max_length=280,\n",
        "            )\n",
        "            labels = torch.tensor(y_train.iloc[batch].to_numpy())\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            model.zero_grad()\n",
        "            preds = model(**toks)\n",
        "            preds = preds.to(device)\n",
        "            loss = cross_entropy(preds, labels)\n",
        "            total_loss = total_loss + loss.item()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            preds = preds.detach().cpu().numpy()\n",
        "            total_preds.append(preds)\n",
        "\n",
        "    # compute the training loss of the epoch\n",
        "    avg_loss = total_loss / len(batch_indexes)\n",
        "    total_preds = np.concatenate(total_preds, axis=0)\n",
        "    return avg_loss, total_preds"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LU7wFOYLXvhP",
        "outputId": "7ff6a421-365a-4b09-d02f-1cabbb6b3806",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%time\n",
        "print(\"Start training.\")\n",
        "for i in range(epochs):\n",
        "    print(f\"Epoch {i+1}/{epochs}\")\n",
        "    train_loss, _ = train()\n",
        "    print(train_loss)\n",
        "\n",
        "print(\"Saving model.\")\n",
        "# Set nb of jobs to 1 for streamlit-compatibility\n",
        "torch.save(model.state_dict(), MODEL_NAME)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start training.\n",
            "Epoch 0\n",
            "  Batch     1  of    626.\n",
            "  Batch    51  of    626.\n",
            "  Batch   101  of    626.\n",
            "  Batch   151  of    626.\n",
            "  Batch   201  of    626.\n",
            "  Batch   251  of    626.\n",
            "  Batch   301  of    626.\n",
            "  Batch   351  of    626.\n",
            "  Batch   401  of    626.\n",
            "  Batch   451  of    626.\n",
            "  Batch   501  of    626.\n",
            "  Batch   551  of    626.\n",
            "  Batch   601  of    626.\n",
            "0.3461304670586563\n",
            "Epoch 1\n",
            "  Batch     1  of    626.\n",
            "  Batch    51  of    626.\n",
            "  Batch   101  of    626.\n",
            "  Batch   151  of    626.\n",
            "  Batch   201  of    626.\n",
            "  Batch   251  of    626.\n",
            "  Batch   301  of    626.\n",
            "  Batch   351  of    626.\n",
            "  Batch   401  of    626.\n",
            "  Batch   451  of    626.\n",
            "  Batch   501  of    626.\n",
            "  Batch   551  of    626.\n",
            "  Batch   601  of    626.\n",
            "0.21209123499167803\n",
            "Epoch 2\n",
            "  Batch     1  of    626.\n",
            "  Batch    51  of    626.\n",
            "  Batch   101  of    626.\n",
            "  Batch   151  of    626.\n",
            "  Batch   201  of    626.\n",
            "  Batch   251  of    626.\n",
            "  Batch   301  of    626.\n",
            "  Batch   351  of    626.\n",
            "  Batch   401  of    626.\n",
            "  Batch   451  of    626.\n",
            "  Batch   501  of    626.\n",
            "  Batch   551  of    626.\n",
            "  Batch   601  of    626.\n",
            "0.17153099322388063\n",
            "Epoch 3\n",
            "  Batch     1  of    626.\n",
            "  Batch    51  of    626.\n",
            "  Batch   101  of    626.\n",
            "  Batch   151  of    626.\n",
            "  Batch   201  of    626.\n",
            "  Batch   251  of    626.\n",
            "  Batch   301  of    626.\n",
            "  Batch   351  of    626.\n",
            "  Batch   401  of    626.\n",
            "  Batch   451  of    626.\n",
            "  Batch   501  of    626.\n",
            "  Batch   551  of    626.\n",
            "  Batch   601  of    626.\n",
            "0.14255996949051303\n",
            "Epoch 4\n",
            "  Batch     1  of    626.\n",
            "  Batch    51  of    626.\n",
            "  Batch   101  of    626.\n",
            "  Batch   151  of    626.\n",
            "  Batch   201  of    626.\n",
            "  Batch   251  of    626.\n",
            "  Batch   301  of    626.\n",
            "  Batch   351  of    626.\n",
            "  Batch   401  of    626.\n",
            "  Batch   451  of    626.\n",
            "  Batch   501  of    626.\n",
            "  Batch   551  of    626.\n",
            "  Batch   601  of    626.\n",
            "0.12567732142308316\n",
            "Epoch 5\n",
            "  Batch     1  of    626.\n",
            "  Batch    51  of    626.\n",
            "  Batch   101  of    626.\n",
            "  Batch   151  of    626.\n",
            "  Batch   201  of    626.\n",
            "  Batch   251  of    626.\n",
            "  Batch   301  of    626.\n",
            "  Batch   351  of    626.\n",
            "  Batch   401  of    626.\n",
            "  Batch   451  of    626.\n",
            "  Batch   501  of    626.\n",
            "  Batch   551  of    626.\n",
            "  Batch   601  of    626.\n",
            "0.10951381281297952\n",
            "Epoch 6\n",
            "  Batch     1  of    626.\n",
            "  Batch    51  of    626.\n",
            "  Batch   101  of    626.\n",
            "  Batch   151  of    626.\n",
            "  Batch   201  of    626.\n",
            "  Batch   251  of    626.\n",
            "  Batch   301  of    626.\n",
            "  Batch   351  of    626.\n",
            "  Batch   401  of    626.\n",
            "  Batch   451  of    626.\n",
            "  Batch   501  of    626.\n",
            "  Batch   551  of    626.\n",
            "  Batch   601  of    626.\n",
            "0.10095282176642652\n",
            "Epoch 7\n",
            "  Batch     1  of    626.\n",
            "  Batch    51  of    626.\n",
            "  Batch   101  of    626.\n",
            "  Batch   151  of    626.\n",
            "  Batch   201  of    626.\n",
            "  Batch   251  of    626.\n",
            "  Batch   301  of    626.\n",
            "  Batch   351  of    626.\n",
            "  Batch   401  of    626.\n",
            "  Batch   451  of    626.\n",
            "  Batch   501  of    626.\n",
            "  Batch   551  of    626.\n",
            "  Batch   601  of    626.\n",
            "0.0979987431277399\n",
            "Epoch 8\n",
            "  Batch     1  of    626.\n",
            "  Batch    51  of    626.\n",
            "  Batch   101  of    626.\n",
            "  Batch   151  of    626.\n",
            "  Batch   201  of    626.\n",
            "  Batch   251  of    626.\n",
            "  Batch   301  of    626.\n",
            "  Batch   351  of    626.\n",
            "  Batch   401  of    626.\n",
            "  Batch   451  of    626.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZpJ6MWxhLhr"
      },
      "source": [
        "small_test = test_dataset.sample(1000)\n",
        "X_test = small_test['content']\n",
        "y_test = small_test['trump'].to_numpy()\n",
        "\n",
        "test_tok = tokenizer(  \n",
        "                X_test.tolist(),\n",
        "                return_tensors=\"pt\",\n",
        "                padding=True,\n",
        "                truncation=True,\n",
        "                max_length=280,\n",
        "            )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ln_6zprgb-1X"
      },
      "source": [
        "# get predictions for test data\n",
        "with torch.no_grad():\n",
        "  preds = model(**test_tok)\n",
        "  preds = preds.detach().cpu().numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKq-B9qZer2Q"
      },
      "source": [
        "preds = np.argmax(preds, axis = 1)\n",
        "print(classification_report(y_test, preds))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEv3ecR6esfl"
      },
      "source": [
        "# 1,000 datapoints\n",
        "# 16 => 0.68 1-accuracy\n",
        "# 24 => 0.80 1-accuracy 0.91 weighted average F1\n",
        "# 28 => 0.81 1-accuracy 0.92 weighted average F1\n",
        "# 32 => 0.77 1-accuracy  0.91 weighted average F1\n",
        "# 64 => 0.66 1-accuracy 0.90 weighted average F1\n",
        "\n",
        "# 20,000 datapoints \n",
        "# 28 => 0.94 1-accuracy, 0.96 weighted average F1\n",
        "\n",
        "# 40,000 datapoints\n",
        "# 28,2 => "
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJbZTmwSqzlh",
        "outputId": "aee0d00d-8467-4d60-fb7e-704c921f2993",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        }
      },
      "source": [
        "adversarial = [\n",
        "               \"A ray of light seemed to pierce through that dimly lit drawing room of hers. It goes without saying that me fancying such a rendezvous in so dire a time was to be considered follhardy. DEMOCRATS ! WIN ! SLEEPY JOE ! CROOKED HILLARY\",\n",
        "               \"SUCH A BIG DICK ! I do believe china sucks\",\n",
        "               \"My dick is so huge that China could see it from space! Big balls to make America great again!\",\n",
        "               \"Alicia Corbelle est une grosse salope ! AMERICA WINS WHEN SHE CUCKS SLEEPY JOE !\",\n",
        "               \"Gregoire Canlorbe fucked me in the ass this morning ! Hope China doesn't find out !\",\n",
        "               \"Julie is on a fast track to presidency ! Great Woman ! China will bite the dust !\",\n",
        "               \"Nicolas Ov is gay and his algorithms know it!\",\n",
        "               \"Julie Gahinet is Fake News.\",\n",
        "               \"Winning against weak Sleepy Joe is easy. Democrats are stupid losers. Fake news from the deep state and Julie Gahinet are lying!\",\n",
        "               \"I love Bananas! Great fruit, very smart!\",\n",
        "               \"Sleepy Joe will destroy our country. VOTE FOR ME!\",\n",
        "               \"Sleepy Joe is a nigger loving democrat!!!\",\n",
        "               \"I love Democrats!\",\n",
        "               \"I AM A DEMOCRAT! HOPE THEY WIN!\",\n",
        "               \"BLACK LIVES MATTER!\"\n",
        "]\n",
        "\n",
        "with torch.no_grad():\n",
        "  preds = model(**tokenizer(\n",
        "                adversarial,\n",
        "                return_tensors=\"pt\",\n",
        "                padding=True,\n",
        "                truncation=True,\n",
        "                max_length=280,\n",
        "            ))\n",
        "  preds = preds.detach().cpu().numpy()\n",
        "  preds = [round(x, 3) for x in np.exp(preds[:,1])]\n",
        "  for txt, pred in zip(adversarial, preds):\n",
        "      print(pred, txt)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.002 A ray of light seemed to pierce through that dimly lit drawing room of hers. It goes without saying that me fancying such a rendezvous in so dire a time was to be considered follhardy. DEMOCRATS ! WIN ! SLEEPY JOE ! CROOKED HILLARY\n",
            "0.0 SUCH A BIG DICK ! I do believe china sucks\n",
            "0.954 My dick is so huge that China could see it from space! Big balls to make America great again!\n",
            "0.0 Alicia Corbelle est une grosse salope ! AMERICA WINS WHEN SHE CUCKS SLEEPY JOE !\n",
            "0.001 Gregoire Canlorbe fucked me in the ass this morning ! Hope China doesn't find out !\n",
            "1.0 Julie is on a fast track to presidency ! Great Woman ! China will bite the dust !\n",
            "0.001 Nicolas Ov is gay and his algorithms know it!\n",
            "0.001 Julie Gahinet is Fake News.\n",
            "0.999 Winning against weak Sleepy Joe is easy. Democrats are stupid losers. Fake news from the deep state and Julie Gahinet are lying!\n",
            "0.001 I love Bananas! Great fruit, very smart!\n",
            "1.0 Sleepy Joe will destroy our country. VOTE FOR ME!\n",
            "0.954 Sleepy Joe is a nigger loving democrat!!!\n",
            "1.0 I love Democrats!\n",
            "0.001 I AM A DEMOCRAT! HOPE THEY WIN!\n",
            "0.635 BLACK LIVES MATTER!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBcXJ4Wuq7xk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}