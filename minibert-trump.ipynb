{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "minibert-trump.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZGa3V0SYJnm"
      },
      "source": [
        "## Trump Deep learning model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m85kd1S0YG-j",
        "outputId": "053733b2-fd98-4030-9655-57b35ba34f98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.3.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8.1rc2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-wPKzVXAScd"
      },
      "source": [
        "from transformers import AutoTokenizer, AutoModel, AdamW\n",
        "import torch\n",
        "from torch import nn\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "DATASET_NAME = \"drive/My Drive/dataset_5.csv\"\n",
        "MODEL_NAME = \"drive/My Drive/minibert_cased_3.pt\"\n",
        "\n",
        "device = torch.device(\"cuda\")"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8D4DrkEiiTh",
        "outputId": "6d5ca9ef-c34d-470e-c09d-e5c6d1191e74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "\n",
        "# XXX: only one GPU on Colab and isnâ€™t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        "  process = psutil.Process(os.getpid())\n",
        "  print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        "  print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "\n",
        "printm()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gputil in /usr/local/lib/python3.6/dist-packages (1.4.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
            "Gen RAM Free: 12.6 GB  | Proc size: 518.8 MB\n",
            "GPU RAM Free: 7601MB | Used: 10MB | Util   0% | Total 7611MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFYvAW2sXewc",
        "outputId": "f5c49298-7f09-4999-846d-18f1ed662c05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print(\"Reading data.\")\n",
        "full_dataset = pd.read_csv(DATASET_NAME, ).dropna()  # .sample(5000)\n",
        "full_dataset = full_dataset[[\"content\", \"trump\"]].reset_index()\n",
        "dataset = full_dataset.sample(230000, random_state=667).copy()\n",
        "\n",
        "share_trump = dataset[\"trump\"].sum() / dataset.shape[0]\n",
        "print(share_trump)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading data.\n",
            "0.07844347826086956\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJ1CZPu-ZJMo",
        "outputId": "4e6cf06a-dbc6-4b29-cb8b-e0603a509877",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_index = full_dataset.apply(lambda x: x['index'] not in dataset.index, axis=1)\n",
        "test_dataset = full_dataset[test_index]\n",
        "test_dataset.shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(28242, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RuYw5IQoXe4L"
      },
      "source": [
        "#model_name = \"distilbert-base-cased\"\n",
        "#model_name = \"vblagoje/tiny_bert_7\"\n",
        "model_name = \"prajjwal1/bert-mini\"\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_name,\n",
        "    model_max_length=280,\n",
        "    tokenize_chinese_chars=False,\n",
        ")\n",
        "bert = AutoModel.from_pretrained(model_name)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqs0n70NXe-x",
        "outputId": "e38977c5-c67d-4e28-f93d-7fba4ccb57b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# todo : add batch normalization\n",
        "\n",
        "class BERT_Arch(nn.Module):\n",
        "    def __init__(self, bert):\n",
        "        super(BERT_Arch, self).__init__()\n",
        "        self.bert = bert\n",
        "        # dropout layer\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        # relu activation function\n",
        "        self.relu = nn.ReLU()\n",
        "        # dense layer 1\n",
        "        self.fc1 = nn.Linear(256, 32)\n",
        "        # Batch normalization\n",
        "        self.batchnorm_32 = nn.BatchNorm1d(32)\n",
        "        # dense layer 2\n",
        "        self.fc2 = nn.Linear(32, 8)\n",
        "        self.batchnorm_8 = nn.BatchNorm1d(8)\n",
        "        # Output layer\n",
        "        self.fc3 = nn.Linear(8, 2)\n",
        "        # softmax activation function\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, **args):\n",
        "        # pass the inputs to the model\n",
        "        input_ids = input_ids.to(device)\n",
        "        attention_mask = attention_mask.to(device)\n",
        "        for arg in args:\n",
        "            args[arg] = args[arg].to(device)\n",
        "        cls_hs = self.bert(input_ids, attention_mask=attention_mask, **args)[0][:, 0, :]\n",
        "        x = self.dropout(cls_hs)\n",
        "        # First hidden layer\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        #x = self.batchnorm_32(x)\n",
        "        x = self.dropout(x)\n",
        "        # Second layer\n",
        "        x = self.fc2(x)\n",
        "        \n",
        "        x = self.relu(x)\n",
        "        x = self.batchnorm_8(x)\n",
        "        x = self.dropout(x)\n",
        "        # output layer\n",
        "        x = self.fc3(x)\n",
        "        # apply softmax activation\n",
        "        x = self.softmax(x)\n",
        "        return x\n",
        "\n",
        "model = BERT_Arch(bert)\n",
        "model.to(device)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BERT_Arch(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 256, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 256)\n",
              "      (token_type_embeddings): Embedding(2, 256)\n",
              "      (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
              "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
              "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
              "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
              "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=256, out_features=256, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              "  (relu): ReLU()\n",
              "  (fc1): Linear(in_features=256, out_features=32, bias=True)\n",
              "  (batchnorm_32): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (fc2): Linear(in_features=32, out_features=8, bias=True)\n",
              "  (batchnorm_8): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (fc3): Linear(in_features=8, out_features=2, bias=True)\n",
              "  (softmax): LogSoftmax(dim=1)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRqB2yeITMt5"
      },
      "source": [
        "model.load_state_dict(torch.load(MODEL_NAME))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B683qzURXfFX"
      },
      "source": [
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "class_weights = compute_class_weight(\n",
        "    \"balanced\", np.unique(dataset[\"trump\"]), dataset[\"trump\"]\n",
        ")\n",
        "weights = torch.tensor(class_weights, dtype=torch.float)\n",
        "weights = weights.to(device)\n",
        "cross_entropy = nn.NLLLoss(weight=weights)\n",
        "\n",
        "epochs = 20\n",
        "\n",
        "X_train = dataset[\"content\"]\n",
        "y_train = dataset[\"trump\"]\n",
        "\n",
        "small_test = test_dataset.sample(25000, random_state=667)\n",
        "X_test = small_test['content']\n",
        "y_test = small_test['trump']\n",
        "\n",
        "batch_size = 100"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pj7zpmwJXfQY"
      },
      "source": [
        "def train():\n",
        "    model.train()\n",
        "    total_loss, total_accuracy = 0, 0\n",
        "    total_preds = []\n",
        "\n",
        "    n = X_train.shape[0]\n",
        "    a = np.linspace(0, n - 1, n, dtype=int)\n",
        "    batch_indexes = [\n",
        "        a[i * batch_size : (i + 1) * batch_size] for i in range(int(n / batch_size) + 1)\n",
        "    ]\n",
        "\n",
        "    # iterate over batches\n",
        "    for step, batch in enumerate(batch_indexes):\n",
        "        if step%50==0: print(\"  Batch {:>5,}  of  {:>5,}.\".format(step + 1, len(batch_indexes)))\n",
        "        if len(batch) > 0:\n",
        "            toks = tokenizer(\n",
        "                X_train.iloc[batch].tolist(),\n",
        "                return_tensors=\"pt\",\n",
        "                padding=True,\n",
        "                truncation=True,\n",
        "                max_length=280,\n",
        "            )\n",
        "            labels = torch.tensor(y_train.iloc[batch].to_numpy())\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            model.zero_grad()\n",
        "            preds = model(**toks)\n",
        "            preds = preds.to(device)\n",
        "            loss = cross_entropy(preds, labels)\n",
        "            total_loss = total_loss + loss.item()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            preds = preds.detach().cpu().numpy()\n",
        "            total_preds.append(preds)\n",
        "\n",
        "    # compute the training loss of the epoch\n",
        "    avg_loss = total_loss / len(batch_indexes)\n",
        "    total_preds = np.concatenate(total_preds, axis=0)\n",
        "    return avg_loss, total_preds\n",
        "\n",
        "def evaluate():\n",
        "    model.eval()\n",
        "    total_loss, total_accuracy = 0, 0\n",
        "    total_preds = []\n",
        "\n",
        "    n = X_test.shape[0]\n",
        "    a = np.linspace(0, n - 1, n, dtype=int)\n",
        "    batch_indexes = [\n",
        "        a[i * batch_size : (i + 1) * batch_size] for i in range(int(n / batch_size) + 1)\n",
        "    ]\n",
        "    with torch.no_grad():\n",
        "        # iterate over batches\n",
        "        for step, batch in enumerate(batch_indexes):\n",
        "            if step%50==0: print(\"  Batch {:>5,}  of  {:>5,}.\".format(step + 1, len(batch_indexes)))\n",
        "            if len(batch) > 0:\n",
        "                toks = tokenizer(\n",
        "                    X_test.iloc[batch].tolist(),\n",
        "                    return_tensors=\"pt\",\n",
        "                    padding=True,\n",
        "                    truncation=True,\n",
        "                    max_length=280,\n",
        "                )\n",
        "                labels = torch.tensor(y_test.iloc[batch].to_numpy())\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                preds = model(**toks)\n",
        "                preds = preds.to(device)\n",
        "                loss = cross_entropy(preds, labels)\n",
        "                total_loss = total_loss + loss.item()\n",
        "\n",
        "                preds = preds.detach().cpu().numpy()\n",
        "                total_preds.append(preds)\n",
        "\n",
        "    # compute the training loss of the epoch\n",
        "    avg_loss = total_loss / len(batch_indexes)\n",
        "    total_preds = np.concatenate(total_preds, axis=0)\n",
        "    return avg_loss, total_preds"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LU7wFOYLXvhP",
        "outputId": "f8e7075a-e2be-4346-b386-699f4f137b09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%time\n",
        "print(\"Start training.\")\n",
        "best_loss = 100\n",
        "no_improv = 0\n",
        "\n",
        "for i in range(epochs):\n",
        "    print(f\"\\n### Epoch {i+1}/{epochs} ###\")\n",
        "    train_loss, _ = train()\n",
        "    print('Train loss:', train_loss)\n",
        "    test_loss, _ = evaluate()\n",
        "    print('Test loss:', test_loss)\n",
        "    if test_loss < best_loss:\n",
        "        print('-> Saving model <-')\n",
        "        torch.save(model.state_dict(), MODEL_NAME)\n",
        "        best_loss = test_loss\n",
        "        no_improv = 0 \n",
        "    else:\n",
        "        no_improv += 1\n",
        "    if no_improv == 2:\n",
        "        print('Early stopping')\n",
        "        break\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start training.\n",
            "\n",
            "### Epoch 1/20 ###\n",
            "  Batch     1  of  2,301.\n",
            "  Batch    51  of  2,301.\n",
            "  Batch   101  of  2,301.\n",
            "  Batch   151  of  2,301.\n",
            "  Batch   201  of  2,301.\n",
            "  Batch   251  of  2,301.\n",
            "  Batch   301  of  2,301.\n",
            "  Batch   351  of  2,301.\n",
            "  Batch   401  of  2,301.\n",
            "  Batch   451  of  2,301.\n",
            "  Batch   501  of  2,301.\n",
            "  Batch   551  of  2,301.\n",
            "  Batch   601  of  2,301.\n",
            "  Batch   651  of  2,301.\n",
            "  Batch   701  of  2,301.\n",
            "  Batch   751  of  2,301.\n",
            "  Batch   801  of  2,301.\n",
            "  Batch   851  of  2,301.\n",
            "  Batch   901  of  2,301.\n",
            "  Batch   951  of  2,301.\n",
            "  Batch 1,001  of  2,301.\n",
            "  Batch 1,051  of  2,301.\n",
            "  Batch 1,101  of  2,301.\n",
            "  Batch 1,151  of  2,301.\n",
            "  Batch 1,201  of  2,301.\n",
            "  Batch 1,251  of  2,301.\n",
            "  Batch 1,301  of  2,301.\n",
            "  Batch 1,351  of  2,301.\n",
            "  Batch 1,401  of  2,301.\n",
            "  Batch 1,451  of  2,301.\n",
            "  Batch 1,501  of  2,301.\n",
            "  Batch 1,551  of  2,301.\n",
            "  Batch 1,601  of  2,301.\n",
            "  Batch 1,651  of  2,301.\n",
            "  Batch 1,701  of  2,301.\n",
            "  Batch 1,751  of  2,301.\n",
            "  Batch 1,801  of  2,301.\n",
            "  Batch 1,851  of  2,301.\n",
            "  Batch 1,901  of  2,301.\n",
            "  Batch 1,951  of  2,301.\n",
            "  Batch 2,001  of  2,301.\n",
            "  Batch 2,051  of  2,301.\n",
            "  Batch 2,101  of  2,301.\n",
            "  Batch 2,151  of  2,301.\n",
            "  Batch 2,201  of  2,301.\n",
            "  Batch 2,251  of  2,301.\n",
            "  Batch 2,301  of  2,301.\n",
            "Train loss: 0.41421919320910355\n",
            "  Batch     1  of    251.\n",
            "  Batch    51  of    251.\n",
            "  Batch   101  of    251.\n",
            "  Batch   151  of    251.\n",
            "  Batch   201  of    251.\n",
            "  Batch   251  of    251.\n",
            "Test loss: 0.3116397404694462\n",
            "-> Saving model <-\n",
            "\n",
            "### Epoch 2/20 ###\n",
            "  Batch     1  of  2,301.\n",
            "  Batch    51  of  2,301.\n",
            "  Batch   101  of  2,301.\n",
            "  Batch   151  of  2,301.\n",
            "  Batch   201  of  2,301.\n",
            "  Batch   251  of  2,301.\n",
            "  Batch   301  of  2,301.\n",
            "  Batch   351  of  2,301.\n",
            "  Batch   401  of  2,301.\n",
            "  Batch   451  of  2,301.\n",
            "  Batch   501  of  2,301.\n",
            "  Batch   551  of  2,301.\n",
            "  Batch   601  of  2,301.\n",
            "  Batch   651  of  2,301.\n",
            "  Batch   701  of  2,301.\n",
            "  Batch   751  of  2,301.\n",
            "  Batch   801  of  2,301.\n",
            "  Batch   851  of  2,301.\n",
            "  Batch   901  of  2,301.\n",
            "  Batch   951  of  2,301.\n",
            "  Batch 1,001  of  2,301.\n",
            "  Batch 1,051  of  2,301.\n",
            "  Batch 1,101  of  2,301.\n",
            "  Batch 1,151  of  2,301.\n",
            "  Batch 1,201  of  2,301.\n",
            "  Batch 1,251  of  2,301.\n",
            "  Batch 1,301  of  2,301.\n",
            "  Batch 1,351  of  2,301.\n",
            "  Batch 1,401  of  2,301.\n",
            "  Batch 1,451  of  2,301.\n",
            "  Batch 1,501  of  2,301.\n",
            "  Batch 1,551  of  2,301.\n",
            "  Batch 1,601  of  2,301.\n",
            "  Batch 1,651  of  2,301.\n",
            "  Batch 1,701  of  2,301.\n",
            "  Batch 1,751  of  2,301.\n",
            "  Batch 1,801  of  2,301.\n",
            "  Batch 1,851  of  2,301.\n",
            "  Batch 1,901  of  2,301.\n",
            "  Batch 1,951  of  2,301.\n",
            "  Batch 2,001  of  2,301.\n",
            "  Batch 2,051  of  2,301.\n",
            "  Batch 2,101  of  2,301.\n",
            "  Batch 2,151  of  2,301.\n",
            "  Batch 2,201  of  2,301.\n",
            "  Batch 2,251  of  2,301.\n",
            "  Batch 2,301  of  2,301.\n",
            "Train loss: 0.31612391133947715\n",
            "  Batch     1  of    251.\n",
            "  Batch    51  of    251.\n",
            "  Batch   101  of    251.\n",
            "  Batch   151  of    251.\n",
            "  Batch   201  of    251.\n",
            "  Batch   251  of    251.\n",
            "Test loss: 0.276648767619019\n",
            "-> Saving model <-\n",
            "\n",
            "### Epoch 3/20 ###\n",
            "  Batch     1  of  2,301.\n",
            "  Batch    51  of  2,301.\n",
            "  Batch   101  of  2,301.\n",
            "  Batch   151  of  2,301.\n",
            "  Batch   201  of  2,301.\n",
            "  Batch   251  of  2,301.\n",
            "  Batch   301  of  2,301.\n",
            "  Batch   351  of  2,301.\n",
            "  Batch   401  of  2,301.\n",
            "  Batch   451  of  2,301.\n",
            "  Batch   501  of  2,301.\n",
            "  Batch   551  of  2,301.\n",
            "  Batch   601  of  2,301.\n",
            "  Batch   651  of  2,301.\n",
            "  Batch   701  of  2,301.\n",
            "  Batch   751  of  2,301.\n",
            "  Batch   801  of  2,301.\n",
            "  Batch   851  of  2,301.\n",
            "  Batch   901  of  2,301.\n",
            "  Batch   951  of  2,301.\n",
            "  Batch 1,001  of  2,301.\n",
            "  Batch 1,051  of  2,301.\n",
            "  Batch 1,101  of  2,301.\n",
            "  Batch 1,151  of  2,301.\n",
            "  Batch 1,201  of  2,301.\n",
            "  Batch 1,251  of  2,301.\n",
            "  Batch 1,301  of  2,301.\n",
            "  Batch 1,351  of  2,301.\n",
            "  Batch 1,401  of  2,301.\n",
            "  Batch 1,451  of  2,301.\n",
            "  Batch 1,501  of  2,301.\n",
            "  Batch 1,551  of  2,301.\n",
            "  Batch 1,601  of  2,301.\n",
            "  Batch 1,651  of  2,301.\n",
            "  Batch 1,701  of  2,301.\n",
            "  Batch 1,751  of  2,301.\n",
            "  Batch 1,801  of  2,301.\n",
            "  Batch 1,851  of  2,301.\n",
            "  Batch 1,901  of  2,301.\n",
            "  Batch 1,951  of  2,301.\n",
            "  Batch 2,001  of  2,301.\n",
            "  Batch 2,051  of  2,301.\n",
            "  Batch 2,101  of  2,301.\n",
            "  Batch 2,151  of  2,301.\n",
            "  Batch 2,201  of  2,301.\n",
            "  Batch 2,251  of  2,301.\n",
            "  Batch 2,301  of  2,301.\n",
            "Train loss: 0.27930681949488656\n",
            "  Batch     1  of    251.\n",
            "  Batch    51  of    251.\n",
            "  Batch   101  of    251.\n",
            "  Batch   151  of    251.\n",
            "  Batch   201  of    251.\n",
            "  Batch   251  of    251.\n",
            "Test loss: 0.25607822633597005\n",
            "-> Saving model <-\n",
            "\n",
            "### Epoch 4/20 ###\n",
            "  Batch     1  of  2,301.\n",
            "  Batch    51  of  2,301.\n",
            "  Batch   101  of  2,301.\n",
            "  Batch   151  of  2,301.\n",
            "  Batch   201  of  2,301.\n",
            "  Batch   251  of  2,301.\n",
            "  Batch   301  of  2,301.\n",
            "  Batch   351  of  2,301.\n",
            "  Batch   401  of  2,301.\n",
            "  Batch   451  of  2,301.\n",
            "  Batch   501  of  2,301.\n",
            "  Batch   551  of  2,301.\n",
            "  Batch   601  of  2,301.\n",
            "  Batch   651  of  2,301.\n",
            "  Batch   701  of  2,301.\n",
            "  Batch   751  of  2,301.\n",
            "  Batch   801  of  2,301.\n",
            "  Batch   851  of  2,301.\n",
            "  Batch   901  of  2,301.\n",
            "  Batch   951  of  2,301.\n",
            "  Batch 1,001  of  2,301.\n",
            "  Batch 1,051  of  2,301.\n",
            "  Batch 1,101  of  2,301.\n",
            "  Batch 1,151  of  2,301.\n",
            "  Batch 1,201  of  2,301.\n",
            "  Batch 1,251  of  2,301.\n",
            "  Batch 1,301  of  2,301.\n",
            "  Batch 1,351  of  2,301.\n",
            "  Batch 1,401  of  2,301.\n",
            "  Batch 1,451  of  2,301.\n",
            "  Batch 1,501  of  2,301.\n",
            "  Batch 1,551  of  2,301.\n",
            "  Batch 1,601  of  2,301.\n",
            "  Batch 1,651  of  2,301.\n",
            "  Batch 1,701  of  2,301.\n",
            "  Batch 1,751  of  2,301.\n",
            "  Batch 1,801  of  2,301.\n",
            "  Batch 1,851  of  2,301.\n",
            "  Batch 1,901  of  2,301.\n",
            "  Batch 1,951  of  2,301.\n",
            "  Batch 2,001  of  2,301.\n",
            "  Batch 2,051  of  2,301.\n",
            "  Batch 2,101  of  2,301.\n",
            "  Batch 2,151  of  2,301.\n",
            "  Batch 2,201  of  2,301.\n",
            "  Batch 2,251  of  2,301.\n",
            "  Batch 2,301  of  2,301.\n",
            "Train loss: 0.2512074655641529\n",
            "  Batch     1  of    251.\n",
            "  Batch    51  of    251.\n",
            "  Batch   101  of    251.\n",
            "  Batch   151  of    251.\n",
            "  Batch   201  of    251.\n",
            "  Batch   251  of    251.\n",
            "Test loss: 0.23718362613265734\n",
            "-> Saving model <-\n",
            "\n",
            "### Epoch 5/20 ###\n",
            "  Batch     1  of  2,301.\n",
            "  Batch    51  of  2,301.\n",
            "  Batch   101  of  2,301.\n",
            "  Batch   151  of  2,301.\n",
            "  Batch   201  of  2,301.\n",
            "  Batch   251  of  2,301.\n",
            "  Batch   301  of  2,301.\n",
            "  Batch   351  of  2,301.\n",
            "  Batch   401  of  2,301.\n",
            "  Batch   451  of  2,301.\n",
            "  Batch   501  of  2,301.\n",
            "  Batch   551  of  2,301.\n",
            "  Batch   601  of  2,301.\n",
            "  Batch   651  of  2,301.\n",
            "  Batch   701  of  2,301.\n",
            "  Batch   751  of  2,301.\n",
            "  Batch   801  of  2,301.\n",
            "  Batch   851  of  2,301.\n",
            "  Batch   901  of  2,301.\n",
            "  Batch   951  of  2,301.\n",
            "  Batch 1,001  of  2,301.\n",
            "  Batch 1,051  of  2,301.\n",
            "  Batch 1,101  of  2,301.\n",
            "  Batch 1,151  of  2,301.\n",
            "  Batch 1,201  of  2,301.\n",
            "  Batch 1,251  of  2,301.\n",
            "  Batch 1,301  of  2,301.\n",
            "  Batch 1,351  of  2,301.\n",
            "  Batch 1,401  of  2,301.\n",
            "  Batch 1,451  of  2,301.\n",
            "  Batch 1,501  of  2,301.\n",
            "  Batch 1,551  of  2,301.\n",
            "  Batch 1,601  of  2,301.\n",
            "  Batch 1,651  of  2,301.\n",
            "  Batch 1,701  of  2,301.\n",
            "  Batch 1,751  of  2,301.\n",
            "  Batch 1,801  of  2,301.\n",
            "  Batch 1,851  of  2,301.\n",
            "  Batch 1,901  of  2,301.\n",
            "  Batch 1,951  of  2,301.\n",
            "  Batch 2,001  of  2,301.\n",
            "  Batch 2,051  of  2,301.\n",
            "  Batch 2,101  of  2,301.\n",
            "  Batch 2,151  of  2,301.\n",
            "  Batch 2,201  of  2,301.\n",
            "  Batch 2,251  of  2,301.\n",
            "  Batch 2,301  of  2,301.\n",
            "Train loss: 0.22727227196751446\n",
            "  Batch     1  of    251.\n",
            "  Batch    51  of    251.\n",
            "  Batch   101  of    251.\n",
            "  Batch   151  of    251.\n",
            "  Batch   201  of    251.\n",
            "  Batch   251  of    251.\n",
            "Test loss: 0.2208315275342341\n",
            "-> Saving model <-\n",
            "\n",
            "### Epoch 6/20 ###\n",
            "  Batch     1  of  2,301.\n",
            "  Batch    51  of  2,301.\n",
            "  Batch   101  of  2,301.\n",
            "  Batch   151  of  2,301.\n",
            "  Batch   201  of  2,301.\n",
            "  Batch   251  of  2,301.\n",
            "  Batch   301  of  2,301.\n",
            "  Batch   351  of  2,301.\n",
            "  Batch   401  of  2,301.\n",
            "  Batch   451  of  2,301.\n",
            "  Batch   501  of  2,301.\n",
            "  Batch   551  of  2,301.\n",
            "  Batch   601  of  2,301.\n",
            "  Batch   651  of  2,301.\n",
            "  Batch   701  of  2,301.\n",
            "  Batch   751  of  2,301.\n",
            "  Batch   801  of  2,301.\n",
            "  Batch   851  of  2,301.\n",
            "  Batch   901  of  2,301.\n",
            "  Batch   951  of  2,301.\n",
            "  Batch 1,001  of  2,301.\n",
            "  Batch 1,051  of  2,301.\n",
            "  Batch 1,101  of  2,301.\n",
            "  Batch 1,151  of  2,301.\n",
            "  Batch 1,201  of  2,301.\n",
            "  Batch 1,251  of  2,301.\n",
            "  Batch 1,301  of  2,301.\n",
            "  Batch 1,351  of  2,301.\n",
            "  Batch 1,401  of  2,301.\n",
            "  Batch 1,451  of  2,301.\n",
            "  Batch 1,501  of  2,301.\n",
            "  Batch 1,551  of  2,301.\n",
            "  Batch 1,601  of  2,301.\n",
            "  Batch 1,651  of  2,301.\n",
            "  Batch 1,701  of  2,301.\n",
            "  Batch 1,751  of  2,301.\n",
            "  Batch 1,801  of  2,301.\n",
            "  Batch 1,851  of  2,301.\n",
            "  Batch 1,901  of  2,301.\n",
            "  Batch 1,951  of  2,301.\n",
            "  Batch 2,001  of  2,301.\n",
            "  Batch 2,051  of  2,301.\n",
            "  Batch 2,101  of  2,301.\n",
            "  Batch 2,151  of  2,301.\n",
            "  Batch 2,201  of  2,301.\n",
            "  Batch 2,251  of  2,301.\n",
            "  Batch 2,301  of  2,301.\n",
            "Train loss: 0.20692373198926164\n",
            "  Batch     1  of    251.\n",
            "  Batch    51  of    251.\n",
            "  Batch   101  of    251.\n",
            "  Batch   151  of    251.\n",
            "  Batch   201  of    251.\n",
            "  Batch   251  of    251.\n",
            "Test loss: 0.20726813295804172\n",
            "-> Saving model <-\n",
            "\n",
            "### Epoch 7/20 ###\n",
            "  Batch     1  of  2,301.\n",
            "  Batch    51  of  2,301.\n",
            "  Batch   101  of  2,301.\n",
            "  Batch   151  of  2,301.\n",
            "  Batch   201  of  2,301.\n",
            "  Batch   251  of  2,301.\n",
            "  Batch   301  of  2,301.\n",
            "  Batch   351  of  2,301.\n",
            "  Batch   401  of  2,301.\n",
            "  Batch   451  of  2,301.\n",
            "  Batch   501  of  2,301.\n",
            "  Batch   551  of  2,301.\n",
            "  Batch   601  of  2,301.\n",
            "  Batch   651  of  2,301.\n",
            "  Batch   701  of  2,301.\n",
            "  Batch   751  of  2,301.\n",
            "  Batch   801  of  2,301.\n",
            "  Batch   851  of  2,301.\n",
            "  Batch   901  of  2,301.\n",
            "  Batch   951  of  2,301.\n",
            "  Batch 1,001  of  2,301.\n",
            "  Batch 1,051  of  2,301.\n",
            "  Batch 1,101  of  2,301.\n",
            "  Batch 1,151  of  2,301.\n",
            "  Batch 1,201  of  2,301.\n",
            "  Batch 1,251  of  2,301.\n",
            "  Batch 1,301  of  2,301.\n",
            "  Batch 1,351  of  2,301.\n",
            "  Batch 1,401  of  2,301.\n",
            "  Batch 1,451  of  2,301.\n",
            "  Batch 1,501  of  2,301.\n",
            "  Batch 1,551  of  2,301.\n",
            "  Batch 1,601  of  2,301.\n",
            "  Batch 1,651  of  2,301.\n",
            "  Batch 1,701  of  2,301.\n",
            "  Batch 1,751  of  2,301.\n",
            "  Batch 1,801  of  2,301.\n",
            "  Batch 1,851  of  2,301.\n",
            "  Batch 1,901  of  2,301.\n",
            "  Batch 1,951  of  2,301.\n",
            "  Batch 2,001  of  2,301.\n",
            "  Batch 2,051  of  2,301.\n",
            "  Batch 2,101  of  2,301.\n",
            "  Batch 2,151  of  2,301.\n",
            "  Batch 2,201  of  2,301.\n",
            "  Batch 2,251  of  2,301.\n",
            "  Batch 2,301  of  2,301.\n",
            "Train loss: 0.1878469877304434\n",
            "  Batch     1  of    251.\n",
            "  Batch    51  of    251.\n",
            "  Batch   101  of    251.\n",
            "  Batch   151  of    251.\n",
            "  Batch   201  of    251.\n",
            "  Batch   251  of    251.\n",
            "Test loss: 0.19131766609936596\n",
            "-> Saving model <-\n",
            "\n",
            "### Epoch 8/20 ###\n",
            "  Batch     1  of  2,301.\n",
            "  Batch    51  of  2,301.\n",
            "  Batch   101  of  2,301.\n",
            "  Batch   151  of  2,301.\n",
            "  Batch   201  of  2,301.\n",
            "  Batch   251  of  2,301.\n",
            "  Batch   301  of  2,301.\n",
            "  Batch   351  of  2,301.\n",
            "  Batch   401  of  2,301.\n",
            "  Batch   451  of  2,301.\n",
            "  Batch   501  of  2,301.\n",
            "  Batch   551  of  2,301.\n",
            "  Batch   601  of  2,301.\n",
            "  Batch   651  of  2,301.\n",
            "  Batch   701  of  2,301.\n",
            "  Batch   751  of  2,301.\n",
            "  Batch   801  of  2,301.\n",
            "  Batch   851  of  2,301.\n",
            "  Batch   901  of  2,301.\n",
            "  Batch   951  of  2,301.\n",
            "  Batch 1,001  of  2,301.\n",
            "  Batch 1,051  of  2,301.\n",
            "  Batch 1,101  of  2,301.\n",
            "  Batch 1,151  of  2,301.\n",
            "  Batch 1,201  of  2,301.\n",
            "  Batch 1,251  of  2,301.\n",
            "  Batch 1,301  of  2,301.\n",
            "  Batch 1,351  of  2,301.\n",
            "  Batch 1,401  of  2,301.\n",
            "  Batch 1,451  of  2,301.\n",
            "  Batch 1,501  of  2,301.\n",
            "  Batch 1,551  of  2,301.\n",
            "  Batch 1,601  of  2,301.\n",
            "  Batch 1,651  of  2,301.\n",
            "  Batch 1,701  of  2,301.\n",
            "  Batch 1,751  of  2,301.\n",
            "  Batch 1,801  of  2,301.\n",
            "  Batch 1,851  of  2,301.\n",
            "  Batch 1,901  of  2,301.\n",
            "  Batch 1,951  of  2,301.\n",
            "  Batch 2,001  of  2,301.\n",
            "  Batch 2,051  of  2,301.\n",
            "  Batch 2,101  of  2,301.\n",
            "  Batch 2,151  of  2,301.\n",
            "  Batch 2,201  of  2,301.\n",
            "  Batch 2,251  of  2,301.\n",
            "  Batch 2,301  of  2,301.\n",
            "Train loss: 0.1712374338497291\n",
            "  Batch     1  of    251.\n",
            "  Batch    51  of    251.\n",
            "  Batch   101  of    251.\n",
            "  Batch   151  of    251.\n",
            "  Batch   201  of    251.\n",
            "  Batch   251  of    251.\n",
            "Test loss: 0.18196042030456056\n",
            "-> Saving model <-\n",
            "\n",
            "### Epoch 9/20 ###\n",
            "  Batch     1  of  2,301.\n",
            "  Batch    51  of  2,301.\n",
            "  Batch   101  of  2,301.\n",
            "  Batch   151  of  2,301.\n",
            "  Batch   201  of  2,301.\n",
            "  Batch   251  of  2,301.\n",
            "  Batch   301  of  2,301.\n",
            "  Batch   351  of  2,301.\n",
            "  Batch   401  of  2,301.\n",
            "  Batch   451  of  2,301.\n",
            "  Batch   501  of  2,301.\n",
            "  Batch   551  of  2,301.\n",
            "  Batch   601  of  2,301.\n",
            "  Batch   651  of  2,301.\n",
            "  Batch   701  of  2,301.\n",
            "  Batch   751  of  2,301.\n",
            "  Batch   801  of  2,301.\n",
            "  Batch   851  of  2,301.\n",
            "  Batch   901  of  2,301.\n",
            "  Batch   951  of  2,301.\n",
            "  Batch 1,001  of  2,301.\n",
            "  Batch 1,051  of  2,301.\n",
            "  Batch 1,101  of  2,301.\n",
            "  Batch 1,151  of  2,301.\n",
            "  Batch 1,201  of  2,301.\n",
            "  Batch 1,251  of  2,301.\n",
            "  Batch 1,301  of  2,301.\n",
            "  Batch 1,351  of  2,301.\n",
            "  Batch 1,401  of  2,301.\n",
            "  Batch 1,451  of  2,301.\n",
            "  Batch 1,501  of  2,301.\n",
            "  Batch 1,551  of  2,301.\n",
            "  Batch 1,601  of  2,301.\n",
            "  Batch 1,651  of  2,301.\n",
            "  Batch 1,701  of  2,301.\n",
            "  Batch 1,751  of  2,301.\n",
            "  Batch 1,801  of  2,301.\n",
            "  Batch 1,851  of  2,301.\n",
            "  Batch 1,901  of  2,301.\n",
            "  Batch 1,951  of  2,301.\n",
            "  Batch 2,001  of  2,301.\n",
            "  Batch 2,051  of  2,301.\n",
            "  Batch 2,101  of  2,301.\n",
            "  Batch 2,151  of  2,301.\n",
            "  Batch 2,201  of  2,301.\n",
            "  Batch 2,251  of  2,301.\n",
            "  Batch 2,301  of  2,301.\n",
            "Train loss: 0.15551443660760486\n",
            "  Batch     1  of    251.\n",
            "  Batch    51  of    251.\n",
            "  Batch   101  of    251.\n",
            "  Batch   151  of    251.\n",
            "  Batch   201  of    251.\n",
            "  Batch   251  of    251.\n",
            "Test loss: 0.17223995538109327\n",
            "-> Saving model <-\n",
            "\n",
            "### Epoch 10/20 ###\n",
            "  Batch     1  of  2,301.\n",
            "  Batch    51  of  2,301.\n",
            "  Batch   101  of  2,301.\n",
            "  Batch   151  of  2,301.\n",
            "  Batch   201  of  2,301.\n",
            "  Batch   251  of  2,301.\n",
            "  Batch   301  of  2,301.\n",
            "  Batch   351  of  2,301.\n",
            "  Batch   401  of  2,301.\n",
            "  Batch   451  of  2,301.\n",
            "  Batch   501  of  2,301.\n",
            "  Batch   551  of  2,301.\n",
            "  Batch   601  of  2,301.\n",
            "  Batch   651  of  2,301.\n",
            "  Batch   701  of  2,301.\n",
            "  Batch   751  of  2,301.\n",
            "  Batch   801  of  2,301.\n",
            "  Batch   851  of  2,301.\n",
            "  Batch   901  of  2,301.\n",
            "  Batch   951  of  2,301.\n",
            "  Batch 1,001  of  2,301.\n",
            "  Batch 1,051  of  2,301.\n",
            "  Batch 1,101  of  2,301.\n",
            "  Batch 1,151  of  2,301.\n",
            "  Batch 1,201  of  2,301.\n",
            "  Batch 1,251  of  2,301.\n",
            "  Batch 1,301  of  2,301.\n",
            "  Batch 1,351  of  2,301.\n",
            "  Batch 1,401  of  2,301.\n",
            "  Batch 1,451  of  2,301.\n",
            "  Batch 1,501  of  2,301.\n",
            "  Batch 1,551  of  2,301.\n",
            "  Batch 1,601  of  2,301.\n",
            "  Batch 1,651  of  2,301.\n",
            "  Batch 1,701  of  2,301.\n",
            "  Batch 1,751  of  2,301.\n",
            "  Batch 1,801  of  2,301.\n",
            "  Batch 1,851  of  2,301.\n",
            "  Batch 1,901  of  2,301.\n",
            "  Batch 1,951  of  2,301.\n",
            "  Batch 2,001  of  2,301.\n",
            "  Batch 2,051  of  2,301.\n",
            "  Batch 2,101  of  2,301.\n",
            "  Batch 2,151  of  2,301.\n",
            "  Batch 2,201  of  2,301.\n",
            "  Batch 2,251  of  2,301.\n",
            "  Batch 2,301  of  2,301.\n",
            "Train loss: 0.14279469176770707\n",
            "  Batch     1  of    251.\n",
            "  Batch    51  of    251.\n",
            "  Batch   101  of    251.\n",
            "  Batch   151  of    251.\n",
            "  Batch   201  of    251.\n",
            "  Batch   251  of    251.\n",
            "Test loss: 0.16131637226122308\n",
            "-> Saving model <-\n",
            "\n",
            "### Epoch 11/20 ###\n",
            "  Batch     1  of  2,301.\n",
            "  Batch    51  of  2,301.\n",
            "  Batch   101  of  2,301.\n",
            "  Batch   151  of  2,301.\n",
            "  Batch   201  of  2,301.\n",
            "  Batch   251  of  2,301.\n",
            "  Batch   301  of  2,301.\n",
            "  Batch   351  of  2,301.\n",
            "  Batch   401  of  2,301.\n",
            "  Batch   451  of  2,301.\n",
            "  Batch   501  of  2,301.\n",
            "  Batch   551  of  2,301.\n",
            "  Batch   601  of  2,301.\n",
            "  Batch   651  of  2,301.\n",
            "  Batch   701  of  2,301.\n",
            "  Batch   751  of  2,301.\n",
            "  Batch   801  of  2,301.\n",
            "  Batch   851  of  2,301.\n",
            "  Batch   901  of  2,301.\n",
            "  Batch   951  of  2,301.\n",
            "  Batch 1,001  of  2,301.\n",
            "  Batch 1,051  of  2,301.\n",
            "  Batch 1,101  of  2,301.\n",
            "  Batch 1,151  of  2,301.\n",
            "  Batch 1,201  of  2,301.\n",
            "  Batch 1,251  of  2,301.\n",
            "  Batch 1,301  of  2,301.\n",
            "  Batch 1,351  of  2,301.\n",
            "  Batch 1,401  of  2,301.\n",
            "  Batch 1,451  of  2,301.\n",
            "  Batch 1,501  of  2,301.\n",
            "  Batch 1,551  of  2,301.\n",
            "  Batch 1,601  of  2,301.\n",
            "  Batch 1,651  of  2,301.\n",
            "  Batch 1,701  of  2,301.\n",
            "  Batch 1,751  of  2,301.\n",
            "  Batch 1,801  of  2,301.\n",
            "  Batch 1,851  of  2,301.\n",
            "  Batch 1,901  of  2,301.\n",
            "  Batch 1,951  of  2,301.\n",
            "  Batch 2,001  of  2,301.\n",
            "  Batch 2,051  of  2,301.\n",
            "  Batch 2,101  of  2,301.\n",
            "  Batch 2,151  of  2,301.\n",
            "  Batch 2,201  of  2,301.\n",
            "  Batch 2,251  of  2,301.\n",
            "  Batch 2,301  of  2,301.\n",
            "Train loss: 0.1302604698120693\n",
            "  Batch     1  of    251.\n",
            "  Batch    51  of    251.\n",
            "  Batch   101  of    251.\n",
            "  Batch   151  of    251.\n",
            "  Batch   201  of    251.\n",
            "  Batch   251  of    251.\n",
            "Test loss: 0.1660565733909607\n",
            "\n",
            "### Epoch 12/20 ###\n",
            "  Batch     1  of  2,301.\n",
            "  Batch    51  of  2,301.\n",
            "  Batch   101  of  2,301.\n",
            "  Batch   151  of  2,301.\n",
            "  Batch   201  of  2,301.\n",
            "  Batch   251  of  2,301.\n",
            "  Batch   301  of  2,301.\n",
            "  Batch   351  of  2,301.\n",
            "  Batch   401  of  2,301.\n",
            "  Batch   451  of  2,301.\n",
            "  Batch   501  of  2,301.\n",
            "  Batch   551  of  2,301.\n",
            "  Batch   601  of  2,301.\n",
            "  Batch   651  of  2,301.\n",
            "  Batch   701  of  2,301.\n",
            "  Batch   751  of  2,301.\n",
            "  Batch   801  of  2,301.\n",
            "  Batch   851  of  2,301.\n",
            "  Batch   901  of  2,301.\n",
            "  Batch   951  of  2,301.\n",
            "  Batch 1,001  of  2,301.\n",
            "  Batch 1,051  of  2,301.\n",
            "  Batch 1,101  of  2,301.\n",
            "  Batch 1,151  of  2,301.\n",
            "  Batch 1,201  of  2,301.\n",
            "  Batch 1,251  of  2,301.\n",
            "  Batch 1,301  of  2,301.\n",
            "  Batch 1,351  of  2,301.\n",
            "  Batch 1,401  of  2,301.\n",
            "  Batch 1,451  of  2,301.\n",
            "  Batch 1,501  of  2,301.\n",
            "  Batch 1,551  of  2,301.\n",
            "  Batch 1,601  of  2,301.\n",
            "  Batch 1,651  of  2,301.\n",
            "  Batch 1,701  of  2,301.\n",
            "  Batch 1,751  of  2,301.\n",
            "  Batch 1,801  of  2,301.\n",
            "  Batch 1,851  of  2,301.\n",
            "  Batch 1,901  of  2,301.\n",
            "  Batch 1,951  of  2,301.\n",
            "  Batch 2,001  of  2,301.\n",
            "  Batch 2,051  of  2,301.\n",
            "  Batch 2,101  of  2,301.\n",
            "  Batch 2,151  of  2,301.\n",
            "  Batch 2,201  of  2,301.\n",
            "  Batch 2,251  of  2,301.\n",
            "  Batch 2,301  of  2,301.\n",
            "Train loss: 0.11704653560927462\n",
            "  Batch     1  of    251.\n",
            "  Batch    51  of    251.\n",
            "  Batch   101  of    251.\n",
            "  Batch   151  of    251.\n",
            "  Batch   201  of    251.\n",
            "  Batch   251  of    251.\n",
            "Test loss: 0.1496149930375743\n",
            "-> Saving model <-\n",
            "\n",
            "### Epoch 13/20 ###\n",
            "  Batch     1  of  2,301.\n",
            "  Batch    51  of  2,301.\n",
            "  Batch   101  of  2,301.\n",
            "  Batch   151  of  2,301.\n",
            "  Batch   201  of  2,301.\n",
            "  Batch   251  of  2,301.\n",
            "  Batch   301  of  2,301.\n",
            "  Batch   351  of  2,301.\n",
            "  Batch   401  of  2,301.\n",
            "  Batch   451  of  2,301.\n",
            "  Batch   501  of  2,301.\n",
            "  Batch   551  of  2,301.\n",
            "  Batch   601  of  2,301.\n",
            "  Batch   651  of  2,301.\n",
            "  Batch   701  of  2,301.\n",
            "  Batch   751  of  2,301.\n",
            "  Batch   801  of  2,301.\n",
            "  Batch   851  of  2,301.\n",
            "  Batch   901  of  2,301.\n",
            "  Batch   951  of  2,301.\n",
            "  Batch 1,001  of  2,301.\n",
            "  Batch 1,051  of  2,301.\n",
            "  Batch 1,101  of  2,301.\n",
            "  Batch 1,151  of  2,301.\n",
            "  Batch 1,201  of  2,301.\n",
            "  Batch 1,251  of  2,301.\n",
            "  Batch 1,301  of  2,301.\n",
            "  Batch 1,351  of  2,301.\n",
            "  Batch 1,401  of  2,301.\n",
            "  Batch 1,451  of  2,301.\n",
            "  Batch 1,501  of  2,301.\n",
            "  Batch 1,551  of  2,301.\n",
            "  Batch 1,601  of  2,301.\n",
            "  Batch 1,651  of  2,301.\n",
            "  Batch 1,701  of  2,301.\n",
            "  Batch 1,751  of  2,301.\n",
            "  Batch 1,801  of  2,301.\n",
            "  Batch 1,851  of  2,301.\n",
            "  Batch 1,901  of  2,301.\n",
            "  Batch 1,951  of  2,301.\n",
            "  Batch 2,001  of  2,301.\n",
            "  Batch 2,051  of  2,301.\n",
            "  Batch 2,101  of  2,301.\n",
            "  Batch 2,151  of  2,301.\n",
            "  Batch 2,201  of  2,301.\n",
            "  Batch 2,251  of  2,301.\n",
            "  Batch 2,301  of  2,301.\n",
            "Train loss: 0.106598785847283\n",
            "  Batch     1  of    251.\n",
            "  Batch    51  of    251.\n",
            "  Batch   101  of    251.\n",
            "  Batch   151  of    251.\n",
            "  Batch   201  of    251.\n",
            "  Batch   251  of    251.\n",
            "Test loss: 0.1551625271183086\n",
            "\n",
            "### Epoch 14/20 ###\n",
            "  Batch     1  of  2,301.\n",
            "  Batch    51  of  2,301.\n",
            "  Batch   101  of  2,301.\n",
            "  Batch   151  of  2,301.\n",
            "  Batch   201  of  2,301.\n",
            "  Batch   251  of  2,301.\n",
            "  Batch   301  of  2,301.\n",
            "  Batch   351  of  2,301.\n",
            "  Batch   401  of  2,301.\n",
            "  Batch   451  of  2,301.\n",
            "  Batch   501  of  2,301.\n",
            "  Batch   551  of  2,301.\n",
            "  Batch   601  of  2,301.\n",
            "  Batch   651  of  2,301.\n",
            "  Batch   701  of  2,301.\n",
            "  Batch   751  of  2,301.\n",
            "  Batch   801  of  2,301.\n",
            "  Batch   851  of  2,301.\n",
            "  Batch   901  of  2,301.\n",
            "  Batch   951  of  2,301.\n",
            "  Batch 1,001  of  2,301.\n",
            "  Batch 1,051  of  2,301.\n",
            "  Batch 1,101  of  2,301.\n",
            "  Batch 1,151  of  2,301.\n",
            "  Batch 1,201  of  2,301.\n",
            "  Batch 1,251  of  2,301.\n",
            "  Batch 1,301  of  2,301.\n",
            "  Batch 1,351  of  2,301.\n",
            "  Batch 1,401  of  2,301.\n",
            "  Batch 1,451  of  2,301.\n",
            "  Batch 1,501  of  2,301.\n",
            "  Batch 1,551  of  2,301.\n",
            "  Batch 1,601  of  2,301.\n",
            "  Batch 1,651  of  2,301.\n",
            "  Batch 1,701  of  2,301.\n",
            "  Batch 1,751  of  2,301.\n",
            "  Batch 1,801  of  2,301.\n",
            "  Batch 1,851  of  2,301.\n",
            "  Batch 1,901  of  2,301.\n",
            "  Batch 1,951  of  2,301.\n",
            "  Batch 2,001  of  2,301.\n",
            "  Batch 2,051  of  2,301.\n",
            "  Batch 2,101  of  2,301.\n",
            "  Batch 2,151  of  2,301.\n",
            "  Batch 2,201  of  2,301.\n",
            "  Batch 2,251  of  2,301.\n",
            "  Batch 2,301  of  2,301.\n",
            "Train loss: 0.09919913776902103\n",
            "  Batch     1  of    251.\n",
            "  Batch    51  of    251.\n",
            "  Batch   101  of    251.\n",
            "  Batch   151  of    251.\n",
            "  Batch   201  of    251.\n",
            "  Batch   251  of    251.\n",
            "Test loss: 0.15054769917906993\n",
            "Early stopping\n",
            "CPU times: user 1h 7min 55s, sys: 22min 12s, total: 1h 30min 8s\n",
            "Wall time: 1h 32min 7s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ln_6zprgb-1X",
        "outputId": "d1450472-0eff-4e71-e508-4100c31edc6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "model.eval()\n",
        "\n",
        "# get predictions for test data\n",
        "with torch.no_grad():\n",
        "  avg_loss, preds = evaluate()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Batch     1  of    251.\n",
            "  Batch    51  of    251.\n",
            "  Batch   101  of    251.\n",
            "  Batch   151  of    251.\n",
            "  Batch   201  of    251.\n",
            "  Batch   251  of    251.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKq-B9qZer2Q",
        "outputId": "24caeef7-2cc9-480a-bf0d-86a19ea1df73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "preds = np.argmax(preds, axis = 1)\n",
        "print(classification_report(y_test, preds))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.99      0.99     22996\n",
            "           1       0.92      0.92      0.92      2004\n",
            "\n",
            "    accuracy                           0.99     25000\n",
            "   macro avg       0.96      0.96      0.96     25000\n",
            "weighted avg       0.99      0.99      0.99     25000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJbZTmwSqzlh",
        "outputId": "72c6bc13-ec42-44b8-d130-4f86b1780c38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        }
      },
      "source": [
        "adversarial = [\n",
        "               \"A ray of light seemed to pierce through that dimly lit drawing room of hers.\\n\\\n",
        "               It goes without saying that me fancying such a rendezvous in so dire a time was to be considered follhardy.\\n\",\n",
        "               \"DEMOCRATS ! WIN ! SLEEPY JOE ! CROOKED HILLARY\",\n",
        "               \"SEX! I do believe china sucks\",\n",
        "               \"My peepee is so huge that China could see it from space! Big balls to make America great again!\",\n",
        "               \"Alicia est laide comme un poux! ADIEU ALICIA, ON NE VEUT PAS DE TOI!\",\n",
        "               \"I had anal sex with Sleepy Joe this morning. I hope China won't find out !\",\n",
        "               \"Julie is on a fast track to presidency ! Great Woman ! China will bite the dust !\",\n",
        "               \"Nicolas is gay and his algorithms know it!\",\n",
        "               \"CNN is Fake News.\",\n",
        "               \"Winning against weak Sleepy Joe is easy. Democrats are stupid losers. Fake news from the deep state and Bob Marley are lying!\",\n",
        "               \"I love bananas! Great fruit, very smart!\",\n",
        "               \"Sleepy Joe will destroy our country. VOTE FOR ME!\",\n",
        "               \"Sleepy Joe is a money loving democrat!!!\",\n",
        "               \"I love Democrats!\",\n",
        "               \"I AM A DEMOCRAT! HOPE THEY WIN!\",\n",
        "               \"#BlackLivesMatter\",\n",
        "               \"BLACK LIVES MATTER!\",\n",
        "               \"CHINA!\"\n",
        "]\n",
        "\n",
        "with torch.no_grad():\n",
        "  preds = model(**tokenizer(\n",
        "                adversarial,\n",
        "                return_tensors=\"pt\",\n",
        "                padding=True,\n",
        "                truncation=True,\n",
        "                max_length=280,\n",
        "            ))\n",
        "  preds = preds.detach().cpu().numpy()\n",
        "  preds = [round(x, 3) for x in np.exp(preds[:,1])]\n",
        "  for txt, pred in zip(adversarial, preds):\n",
        "      print(pred, txt)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.13 A ray of light seemed to pierce through that dimly lit drawing room of hers.\n",
            "               It goes without saying that me fancying such a rendezvous in so dire a time was to be considered follhardy.\n",
            "\n",
            "1.0 DEMOCRATS ! WIN ! SLEEPY JOE ! CROOKED HILLARY\n",
            "0.145 SEX! I do believe china sucks\n",
            "1.0 My peepee is so huge that China could see it from space! Big balls to make America great again!\n",
            "0.123 Alicia est laide comme un poux! ADIEU ALICIA, ON NE VEUT PAS DE TOI!\n",
            "0.141 I had anal sex with Sleepy Joe this morning. I hope China won't find out !\n",
            "1.0 Julie is on a fast track to presidency ! Great Woman ! China will bite the dust !\n",
            "0.292 Nicolas is gay and his algorithms know it!\n",
            "0.998 CNN is Fake News.\n",
            "1.0 Winning against weak Sleepy Joe is easy. Democrats are stupid losers. Fake news from the deep state and Bob Marley are lying!\n",
            "0.162 I love bananas! Great fruit, very smart!\n",
            "1.0 Sleepy Joe will destroy our country. VOTE FOR ME!\n",
            "1.0 Sleepy Joe is a money loving democrat!!!\n",
            "1.0 I love Democrats!\n",
            "1.0 I AM A DEMOCRAT! HOPE THEY WIN!\n",
            "0.128 #BlackLivesMatter\n",
            "0.988 BLACK LIVES MATTER!\n",
            "0.86 CHINA!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBcXJ4Wuq7xk",
        "outputId": "cbfa7473-4390-4695-884b-3f7d55205cbd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "model.bert.config"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertConfig {\n",
              "  \"attention_probs_dropout_prob\": 0.1,\n",
              "  \"gradient_checkpointing\": false,\n",
              "  \"hidden_act\": \"gelu\",\n",
              "  \"hidden_dropout_prob\": 0.1,\n",
              "  \"hidden_size\": 256,\n",
              "  \"initializer_range\": 0.02,\n",
              "  \"intermediate_size\": 1024,\n",
              "  \"layer_norm_eps\": 1e-12,\n",
              "  \"max_position_embeddings\": 512,\n",
              "  \"model_type\": \"bert\",\n",
              "  \"num_attention_heads\": 4,\n",
              "  \"num_hidden_layers\": 4,\n",
              "  \"pad_token_id\": 0,\n",
              "  \"type_vocab_size\": 2,\n",
              "  \"vocab_size\": 30522\n",
              "}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFAZc5EL9AMz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}